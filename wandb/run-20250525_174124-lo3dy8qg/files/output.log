/opt/anaconda3/envs/dart_env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
05/25 05:41:24 PM epoch 0 lr 9.999753e-03
05/25 05:41:24 PM genotype = Genotype(normal=[('dil_conv_5x5', 1), ('sep_conv_3x3', 0), ('sep_conv_3x3', 2), ('sep_conv_3x3', 0), ('dil_conv_5x5', 0), ('max_pool_3x3', 3), ('dil_conv_5x5', 0), ('dil_conv_5x5', 4), ('skip_connect', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 0), ('skip_connect', 2), ('max_pool_3x3', 3), ('max_pool_3x3', 1), ('max_pool_3x3', 3), ('max_pool_3x3', 2), ('dil_conv_5x5', 1), ('conv_1x1', 0), ('dil_conv_5x5', 2), ('conv_1x1', 1), ('dil_conv_5x5', 0), ('conv_1x1', 3), ('conv_1x1', 2), ('dil_conv_5x5', 4)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 0), ('conv_3x3', 1), ('sep_conv_3x3', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 2), ('sep_conv_3x3', 3), ('sep_conv_3x3', 3), ('sep_conv_3x3', 1), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 0), ('skip_connect', 2), ('skip_connect', 0), ('skip_connect', 2), ('conv_1x1', 1), ('conv_1x1', 0), ('dil_conv_5x5', 2), ('conv_1x1', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 3), ('conv_1x1', 4), ('conv_1x1', 2)], reduce_concat=range(2, 6))
05/25 05:41:24 PM Softmax of alphas_normal:
05/25 05:41:26 PM Cell 0: tensor([[0.2496, 0.2502, 0.2501, 0.2500],
        [0.2500, 0.2498, 0.2502, 0.2500],
        [0.2496, 0.2504, 0.2501, 0.2499],
        [0.2497, 0.2501, 0.2501, 0.2501],
        [0.2497, 0.2505, 0.2505, 0.2493],
        [0.2498, 0.2502, 0.2503, 0.2497],
        [0.2497, 0.2501, 0.2501, 0.2501],
        [0.2501, 0.2498, 0.2499, 0.2501],
        [0.2500, 0.2500, 0.2497, 0.2503],
        [0.2501, 0.2497, 0.2505, 0.2497],
        [0.2500, 0.2502, 0.2498, 0.2499],
        [0.2498, 0.2500, 0.2501, 0.2501],
        [0.2500, 0.2502, 0.2499, 0.2499],
        [0.2498, 0.2501, 0.2504, 0.2497]], device='mps:0',
       grad_fn=<SoftmaxBackward0>)
05/25 05:41:27 PM Cell 1: tensor([[0.5001, 0.4999],
        [0.4998, 0.5002],
        [0.5003, 0.4997],
        [0.5000, 0.5000],
        [0.4999, 0.5001],
        [0.5002, 0.4998],
        [0.5003, 0.4997],
        [0.4999, 0.5001],
        [0.5003, 0.4997],
        [0.4999, 0.5001],
        [0.4997, 0.5003],
        [0.5005, 0.4995],
        [0.5005, 0.4995],
        [0.5000, 0.5000]], device='mps:0', grad_fn=<SoftmaxBackward0>)
05/25 05:41:27 PM Cell 2: tensor([[0.4996, 0.5004],
        [0.5004, 0.4996],
        [0.4999, 0.5001],
        [0.4996, 0.5004],
        [0.5005, 0.4995],
        [0.5005, 0.4995],
        [0.5002, 0.4998],
        [0.5000, 0.5000],
        [0.4997, 0.5003],
        [0.5002, 0.4998],
        [0.5002, 0.4998],
        [0.4993, 0.5007],
        [0.4997, 0.5003],
        [0.5004, 0.4996]], device='mps:0', grad_fn=<SoftmaxBackward0>)
05/25 05:41:27 PM Softmax of alphas_reduce:
05/25 05:41:27 PM Cell 0: tensor([[0.2500, 0.2500, 0.2498, 0.2502],
        [0.2502, 0.2499, 0.2500, 0.2499],
        [0.2499, 0.2501, 0.2499, 0.2501],
        [0.2498, 0.2505, 0.2498, 0.2498],
        [0.2499, 0.2501, 0.2500, 0.2500],
        [0.2499, 0.2496, 0.2502, 0.2503],
        [0.2500, 0.2501, 0.2501, 0.2498],
        [0.2500, 0.2499, 0.2496, 0.2505],
        [0.2497, 0.2503, 0.2502, 0.2498],
        [0.2500, 0.2501, 0.2497, 0.2502],
        [0.2499, 0.2503, 0.2503, 0.2495],
        [0.2501, 0.2497, 0.2501, 0.2501],
        [0.2499, 0.2504, 0.2500, 0.2497],
        [0.2502, 0.2499, 0.2501, 0.2498]], device='mps:0',
       grad_fn=<SoftmaxBackward0>)
05/25 05:41:27 PM Cell 1: tensor([[0.5001, 0.4999],
        [0.5002, 0.4998],
        [0.5006, 0.4994],
        [0.5010, 0.4990],
        [0.5002, 0.4998],
        [0.5007, 0.4993],
        [0.4998, 0.5002],
        [0.4996, 0.5004],
        [0.4996, 0.5004],
        [0.4994, 0.5006],
        [0.4997, 0.5003],
        [0.4996, 0.5004],
        [0.5003, 0.4997],
        [0.5000, 0.5000]], device='mps:0', grad_fn=<SoftmaxBackward0>)
05/25 05:41:27 PM Cell 2: tensor([[0.4996, 0.5004],
        [0.4995, 0.5005],
        [0.4997, 0.5003],
        [0.5000, 0.5000],
        [0.5003, 0.4997],
        [0.4999, 0.5001],
        [0.5002, 0.4998],
        [0.5006, 0.4994],
        [0.5006, 0.4994],
        [0.5001, 0.4999],
        [0.4999, 0.5001],
        [0.4996, 0.5004],
        [0.4996, 0.5004],
        [0.4990, 0.5010]], device='mps:0', grad_fn=<SoftmaxBackward0>)
Traceback (most recent call last):                                              
  File "/Users/duongviethuy/Working_space/Nas_energy/Differential_Nas_energy_awareness/train_search.py", line 401, in <module>
    main()
  File "/Users/duongviethuy/Working_space/Nas_energy/Differential_Nas_energy_awareness/train_search.py", line 219, in main
    train_top1, train_top5, train_objs, energy =train(train_queue, valid_queue, model, architect, criterion, optimizer, lr, device,args)
                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/duongviethuy/Working_space/Nas_energy/Differential_Nas_energy_awareness/train_search.py", line 282, in train
    architect.step(input_train, target_train, input_search, target_search, lr, optimizer, args.unrolled)
TypeError: Architect.step() missing 1 required positional argument: 'rate'
