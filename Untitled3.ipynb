{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89dc233-8fa4-4a26-bbc7-3c3d09aba3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "import logging\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torch.backends.cudnn as cudnn\n",
    "from model import NetworkCIFAR as Network\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, Subset\n",
    "\n",
    "\n",
    "\n",
    "Genotype = namedtuple('Genotype', 'normal normal_concat reduce reduce_concat')\n",
    "SNN_DARTS = Genotype(normal=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('snn_multistep_3x3', 1), ('snn_multistep_3x3', 1), ('snn_multistep_5x5', 3), ('max_pool_3x3', 0), ('snn_multistep_3x3', 4)], normal_concat=range(2, 6), reduce=[('avg_pool_3x3', 1), ('snn_multistep_5x5', 0), ('skip_connect', 0), ('snn_multistep_3x3', 2), ('skip_connect', 3), ('max_pool_3x3', 2), ('snn_multistep_5x5', 2), ('snn_multistep_3x3', 3)], reduce_concat=range(2, 6)) #mIoU: 0.8560 | Loss: 0.1157 | Avg MACs: 280.51 e-6\n",
    "\n",
    "SNN_DARTS = Genotype(normal=[('sep_conv_3x3', 1), ('max_pool_3x3', 0), ('dil_conv_3x3', 2), ('sep_conv_3x3', 0), ('sep_conv_3x3', 0), ('sep_conv_3x3', 1), ('dil_conv_3x3', 2), ('dil_conv_3x3', 4)], normal_concat=range(2, 6), reduce=[('skip_connect', 1), ('skip_connect', 0), ('dil_conv_3x3', 0), ('sep_conv_5x5', 2), ('sep_conv_3x3', 0), ('sep_conv_5x5', 3), ('sep_conv_5x5', 2), ('sep_conv_3x3', 3)], reduce_concat=range(2, 6))\n",
    "\n",
    "\n",
    "# New_dart= Genotype(normal=[('conv_1x1', 0), ('conv_1x1', 1), ('dil_conv_3x3', 2), ('grouped_conv', 1), ('max_pool_3x3', 3), ('conv_1x1', 2), ('alt_sep_conv', 3), ('conv_1x1', 4)], normal_concat=range(2, 6), reduce=[('skip_connect', 1), ('conv_1x1', 0), ('dil_conv_5x5', 1), ('skip_connect', 0), ('grouped_conv', 1), ('sep_conv_5x5', 3), ('grouped_conv', 1), ('sep_conv_3x3', 2)], reduce_concat=range(2, 6))\n",
    "\n",
    "# 05/17 03:02:29 PM Validation mIoU: 0.9393 | Val F1: 0.0092  | Loss: 0.1768 | Avg MACs: tensor(0.0026, device='cuda:0')\n",
    "# 05/17 03:02:29 PM epoch 6 lr 1.000000e-03\n",
    "New_dart = Genotype(normal=[('skip_connect', 1), ('skip_connect', 0), ('skip_connect', 2), ('skip_connect', 0), ('skip_connect', 3), ('skip_connect', 1), ('skip_connect', 2), ('skip_connect', 3)], normal_concat=range(2, 6), reduce=[('grouped_conv', 1), ('grouped_conv', 0), ('skip_connect', 2), ('grouped_conv', 0), ('skip_connect', 3), ('skip_connect', 2), ('skip_connect', 4), ('skip_connect', 2)], reduce_concat=range(2, 6))\n",
    "\n",
    "\n",
    "# 05/18 11:59:55 AM train mIoU: 0.9416 | train F1: 0.0207  | Loss: 0.1833 | Avg MACs: 0.0015698829665780067\n",
    "# 05/18 11:59:57 AM Validation mIoU: 0.9385 | Val F1: 0.0713  | Loss: 0.1995 | Avg MACs: tensor(0.0016, device='cuda:0')\n",
    "# 05/18 11:59:58 AM epoch 8 lr 1.000000e-03\n",
    "New_dart = Genotype(normal=[('skip_connect', 1), ('skip_connect', 0), ('skip_connect', 2), ('skip_connect', 1), ('skip_connect', 3), ('skip_connect', 2), ('skip_connect', 3), ('skip_connect', 2)], normal_concat=range(2, 6), reduce=[('max_pool_3x3', 1), ('max_pool_3x3', 0), ('skip_connect', 2), ('max_pool_3x3', 1), ('skip_connect', 3), ('skip_connect', 2), ('skip_connect', 4), ('skip_connect', 3)], reduce_concat=range(2, 6))\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class WeightedFocalDiceLoss(nn.Module):\n",
    "    def __init__(self, class_frequencies, gamma=2.0, alpha=0.25, bce_weight=0.5, dice_weight=0.5, min_weight=0.1, max_weight=50.0, rare_class_factor=10.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.bce_weight = bce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        # Compute weights based on class frequencies\n",
    "        freq_tensor = torch.tensor(class_frequencies, dtype=torch.float32)\n",
    "        inverse_freq = 1.0 / (freq_tensor + 1e-6)\n",
    "        weights = inverse_freq.clone()\n",
    "        weights[:-1] = weights[:-1] * rare_class_factor  # Tăng hệ số cho các lớp không phải nền\n",
    "        weights = torch.clamp(weights, min_weight / freq_tensor.max(), max_weight / freq_tensor.min())\n",
    "        weights = weights / weights.sum() * len(weights)  # Chuẩn hóa\n",
    "        self.weights = weights\n",
    "        if torch.cuda.is_available():\n",
    "            self.weights = self.weights.cuda()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Apply sigmoid to logits to get probabilities\n",
    "        logits = logits / (torch.abs(logits).max() + 1e-6)  # Normalize logits\n",
    "        probs = torch.sigmoid(logits)\n",
    "        targets = targets.float()\n",
    "\n",
    "        # Ensure shapes are compatible\n",
    "        if targets.dim() == 4 and targets.shape[1] == 1:  # Single-label case, convert to one-hot\n",
    "            num_classes = logits.shape[1]\n",
    "            targets_one_hot = torch.zeros_like(logits).scatter_(1, targets.long(), 1.0)\n",
    "            targets = targets_one_hot\n",
    "\n",
    "        # Compute Focal Loss\n",
    "        bce = - (targets * torch.log(probs + 1e-6) + (1 - targets) * torch.log(1 - probs + 1e-6))\n",
    "        pt = torch.where(targets == 1, probs, 1 - probs)\n",
    "        focal_term = (1 - pt) ** self.gamma\n",
    "        focal_loss = focal_term * bce\n",
    "        weights_expanded = self.weights.view(1, -1, 1, 1).expand_as(focal_loss)\n",
    "        focal_loss_weighted = focal_loss * weights_expanded\n",
    "        focal_loss_final = focal_loss_weighted.mean()\n",
    "\n",
    "        # Compute Dice Loss\n",
    "        intersection = (probs * targets).sum(dim=(0, 2, 3))\n",
    "        union = probs.sum(dim=(0, 2, 3)) + targets.sum(dim=(0, 2, 3))\n",
    "        dice_loss = 1 - (2.0 * intersection + 1e-6) / (union + 1e-6)\n",
    "        dice_loss = (dice_loss * self.weights).mean()\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = self.bce_weight * focal_loss_final + self.dice_weight * dice_loss\n",
    "        return total_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, weights=None, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.weights = weights\n",
    "        self.reduction = reduction\n",
    "        if weights is not None:\n",
    "            self.weights = torch.tensor(weights, dtype=torch.float32)\n",
    "    def forward(self, logits, targets):\n",
    "        # Áp dụng sigmoid để chuyển logits thành xác suất\n",
    "        self.weights=self.weights.to(logits.device)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        targets = targets.float()\n",
    "\n",
    "        # Đảm bảo kích thước đầu vào phù hợp\n",
    "        if targets.dim() == 1:  # Nếu nhãn là 1D (một mẫu), mở rộng thành [1, C]\n",
    "            targets = targets.unsqueeze(0)\n",
    "        if logits.dim() == 1:  # Nếu logits là 1D (một mẫu), mở rộng thành [1, C]\n",
    "            logits = logits.unsqueeze(0)\n",
    "        if targets.size(1) != logits.size(1):\n",
    "            raise ValueError(f\"Số lớp trong targets ({targets.size(1)}) phải khớp với logits ({logits.size(1)})\")\n",
    "\n",
    "        # Tính BCE cho từng lớp\n",
    "        bce = - (targets * torch.log(probs + 1e-6) + (1 - targets) * torch.log(1 - probs + 1e-6))\n",
    "\n",
    "        # Áp dụng trọng số nếu có\n",
    "        if self.weights is not None:\n",
    "            if len(self.weights) != bce.size(1):\n",
    "                raise ValueError(f\"Độ dài của weights ({len(self.weights)}) phải khớp với số lớp ({bce.size(1)})\")\n",
    "            weights_expanded = self.weights.view(1, -1).expand_as(bce)\n",
    "            bce_weighted = bce * weights_expanded\n",
    "            if self.reduction == 'mean':\n",
    "                loss = bce_weighted.mean()\n",
    "            elif self.reduction == 'sum':\n",
    "                loss = bce_weighted.sum()\n",
    "            else:\n",
    "                loss = bce_weighted\n",
    "        else:\n",
    "            if self.reduction == 'mean':\n",
    "                loss = bce.mean()\n",
    "            elif self.reduction == 'sum':\n",
    "                loss = bce.sum()\n",
    "            else:\n",
    "                loss = bce\n",
    "\n",
    "        return loss\n",
    "\n",
    "def analyze_labels(dataset, num_classes=8):\n",
    "    \"\"\"Analyze label frequency in the dataset for multilabel classification.\"\"\"\n",
    "    class_counts = np.zeros(num_classes)\n",
    "    total_samples = 0\n",
    "\n",
    "    # Duyệt qua tập dữ liệu để đếm số lần xuất hiện của từng lớp\n",
    "    for _, label in tqdm(dataset, desc=\"Analyzing labels\"):\n",
    "        # label là tensor 1D có shape [num_classes] với giá trị 0 hoặc 1\n",
    "        class_counts += label.cpu().numpy()  # Cộng dồn số lần xuất hiện của từng lớp\n",
    "        total_samples += 1\n",
    "\n",
    "    # Tính tần suất của từng lớp\n",
    "    class_frequencies = class_counts / total_samples\n",
    "    logging.info(\"Class frequencies: %s\", class_frequencies)\n",
    "    logging.info(\"Total samples analyzed: %d\", total_samples)\n",
    "\n",
    "    # Tính trọng số dựa trên nghịch đảo tần suất\n",
    "    weights = 1.0 / (class_frequencies + 1e-6)  # Tránh chia cho 0\n",
    "    weights = weights / weights.sum() * num_classes  # Chuẩn hóa để tổng trọng số bằng num_classes\n",
    "    logging.info(\"Computed weights: %s\", weights)\n",
    "\n",
    "    return class_frequencies.tolist(), weights.tolist()\n",
    "    \n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        targets = targets.float()\n",
    "        intersection = (probs * targets).sum(dim=(2, 3))\n",
    "        union = probs.sum(dim=(2, 3)) + targets.sum(dim=(2, 3))\n",
    "        dice = (2 * intersection + self.eps) / (union + self.eps)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "class CustomPTSegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        super().__init__()\n",
    "        self.images_dir = os.path.join(root_dir, \"X\")\n",
    "        self.labels_dir = os.path.join(root_dir, \"vector_label\")\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(self.images_dir, '*.pt')))\n",
    "        self.label_paths = [os.path.join(self.labels_dir, os.path.basename(p)) for p in self.image_paths]\n",
    "        assert all([os.path.exists(p) for p in self.label_paths]), \"Missing label files.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.load(self.image_paths[idx]).float()  # Normalize\n",
    "        label = torch.load(self.label_paths[idx]).float()\n",
    "        # image = image.permute(0, 2, 1)  # [C, H, W]\n",
    "        # label = label.permute(0, 2, 1)\n",
    "        label = label[:8]\n",
    "        if self.transform:\n",
    "            image, label = self.transform(image, label)\n",
    "\n",
    "        # print(f\"size anh : {image.size()}, {label.size()}\")\n",
    "        return image, label\n",
    "\n",
    "parser = argparse.ArgumentParser(\"segmentation\")\n",
    "parser.add_argument('--batch_size', type=int, default=16)\n",
    "parser.add_argument('--learning_rate', type=float, default=0.001)\n",
    "parser.add_argument('--epochs', type=int, default=50)\n",
    "parser.add_argument('--gpu', type=int, default=0)\n",
    "parser.add_argument('--grad_clip', type=float, default=5)\n",
    "parser.add_argument('--save', type=str, default='EXP')\n",
    "parser.add_argument('--arch', type=str, default='DARTS')\n",
    "parser.add_argument('--seed', type=int, default=2)\n",
    "parser.add_argument('--layers', type=int, default=2)\n",
    "parser.add_argument('--drop_path_prob', type=float, default=0.2, help='drop path probability')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "args.save = './log_train/'  \n",
    "os.makedirs(args.save, exist_ok=True)\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_file_path = os.path.join(args.save, f\"log_{timestamp}.txt\")\n",
    "\n",
    "log_format = '%(asctime)s %(message)s'\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,\n",
    "    level=logging.INFO,\n",
    "    format=log_format,\n",
    "    datefmt='%m/%d %I:%M:%S %p'\n",
    ")\n",
    "fh = logging.FileHandler(log_file_path)  # lưu log ra file log.txt trong thư mục hiện tại\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        targets = targets.float()\n",
    "        intersection = (probs * targets).sum(dim=(2, 3))\n",
    "        union = probs.sum(dim=(2, 3)) + targets.sum(dim=(2, 3))\n",
    "        dice = (2 * intersection + self.eps) / (union + self.eps)\n",
    "        return 1 - dice.mean()\n",
    "class WeightedFocalLoss(nn.Module):\n",
    "    def __init__(self, class_frequencies, gamma=2.0, alpha=0.25):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.weights = torch.tensor(class_frequencies).reciprocal()  # Keep on CPU\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        weights = self.weights.to(logits.device)  # Move to GPU during forward\n",
    "        bce_loss = nn.BCEWithLogitsLoss(weight=weights)(logits, targets)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)\n",
    "        focal_loss = -self.alpha * (1 - p_t) ** self.gamma * torch.log(p_t + 1e-6)\n",
    "        return focal_loss.mean()\n",
    "\n",
    "def compute_metrics(preds, labels, threshold=0.5, double_indices=[0,2,4,5,6]):\n",
    "    preds = torch.sigmoid(preds)\n",
    "    if double_indices is not None:\n",
    "        # double_indices: danh sách hoặc tensor các chỉ số cần nhân\n",
    "        mask = torch.zeros_like(preds)\n",
    "        mask[:, double_indices] = 1\n",
    "        preds = preds * (1 + mask)\n",
    "    preds = preds > threshold\n",
    "    labels = labels > 0.5\n",
    "    tp = (preds & labels).sum(dim=0).float()\n",
    "    fp = (preds & ~labels).sum(dim=0).float()\n",
    "    fn = (~preds & labels).sum(dim=0).float()\n",
    "    precision = tp / (tp + fp + 1e-6)\n",
    "    recall = tp / (tp + fn + 1e-6)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "    accuracy = (preds == labels).float().mean(dim=0)\n",
    "    return {\n",
    "        'accuracy': accuracy.mean().item(),\n",
    "        'f1_macro': f1.mean().item(),\n",
    "        'precision': 0,\n",
    "        'recall': 0\n",
    "    }\n",
    "def main():\n",
    "    device = torch.device(f\"cuda:{args.gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    cudnn.benchmark = True\n",
    "    cudnn.enabled = True\n",
    "\n",
    "    genotype = New_dart\n",
    "    model = Network(36, 10,args.layers, False, genotype,4,1,device).to(device)\n",
    "\n",
    "\n",
    "    train_transform, valid_transform = utils._data_transforms_cifar10(args)\n",
    "    train_data = dset.CIFAR10(root=args.data, train=True, download=True, transform=train_transform)\n",
    "\n",
    "    num_train = len(train_data)\n",
    "    # num_train=500\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(args.train_portion * num_train))\n",
    "\n",
    "    train_queue = torch.utils.data.DataLoader(\n",
    "      train_data, batch_size=args.batch_size,\n",
    "sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),\n",
    "      pin_memory=True, num_workers=2)\n",
    "\n",
    "    valid_queue = torch.utils.data.DataLoader(\n",
    "      train_data, batch_size=args.batch_size,\n",
    "      sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[split:num_train]),\n",
    "      pin_memory=True, num_workers=2)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        args.learning_rate,\n",
    "        momentum=args.momentum,\n",
    "        weight_decay=args.weight_decay\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "    for epoch in range(args.epochs):\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        logging.info(\"Epoch %d: Learning Rate %.6f\", epoch, current_lr)\n",
    "        # train_loss, train_metrics = train(train_loader, model, criterion, optimizer, device, epoch)\n",
    "        # val_loss, val_metrics = evaluate(val_loader, model, criterion, device)\n",
    "        # logging.info(\"Epoch %d: Train Loss %.4f | Val Loss %.4f | Train F1 %.4f | Val F1 %.4f | Train Acc %.4f | Val Acc %.4f\",\n",
    "        #              epoch, train_loss, val_loss, train_metrics['f1_macro'], val_metrics['f1_macro'],\n",
    "        #              train_metrics['accuracy'], val_metrics['accuracy'])\n",
    "        # wandb.log({\n",
    "        #     \"epoch\": epoch,\n",
    "        #     \"train/loss\": train_loss,\n",
    "        #     \"train/f1_macro\": train_metrics['f1_macro'],\n",
    "        #     \"train/accuracy\": train_metrics['accuracy'],\n",
    "        #     \"train/precision\": train_metrics['precision'],\n",
    "        #     \"train/recall\": train_metrics['recall'],\n",
    "        #     \"val/loss\": val_loss,\n",
    "        #     \"val/f1_macro\": val_metrics['f1_macro'],\n",
    "        #     \"val/accuracy\": val_metrics['accuracy'],\n",
    "        #     \"val/precision\": val_metrics['precision'],\n",
    "        #     \"val/recall\": val_metrics['recall'],\n",
    "        #     \"lr\": current_lr\n",
    "        # })\n",
    "\n",
    "        train_top1, train_top5, train_objs =train(train_loader, model,  criterion, optimizer, device ,args)\n",
    "        logging.info(\"train Top 1: %.4f | train Top 5: %.4f  | Loss: %.4f | Total energy: %s\", train_top1, train_top5, train_objs)\n",
    "\n",
    "        valid_top1, valid_top5, valid_objs, =infer(val_loader, model, criterion, device)\n",
    "        \n",
    "        logging.info(\"Validation Top 1: %.4f | Val Top 5: %.4f  | Loss: %.4f\",valid_top1,  valid_top5, valid_objs)\n",
    "        wandb.log({\n",
    "            \"epoch/train_Top1\": train_top1,\n",
    "            \"epoch/train_loss\": train_objs,\n",
    "            \"epoch/train_Top5\":train_top5,\n",
    "            \"epoch/val_Top1\": valid_top1,\n",
    "            \"epoch/val_loss\": valid_objs,\n",
    "            \"epoch/valid_top5\":valid_top5,\n",
    "            \"epoch\": epoch\n",
    "        })\n",
    "\n",
    "        \n",
    "        torch.save(model.state_dict(), os.path.join(args.save, f\"model_{epoch}.pt\"))\n",
    "        scheduler.step()\n",
    "\n",
    "def train(loader, model, criterion, optimizer, device, agrs):\n",
    "    epoch=args.epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_metrics = {'accuracy': 0, 'f1_macro': 0, 'precision': 0, 'recall': 0}\n",
    "    total_samples = 0\n",
    "    objs = utils.AvgrageMeter()\n",
    "    # metrics_avg = {'accuracy': utils.AvgrageMeter(), 'f1_macro': utils.AvgrageMeter()}\n",
    "    objs = utils.AvgrageMeter()\n",
    "    top1 = utils.AvgrageMeter()\n",
    "    top5 = utils.AvgrageMeter()\n",
    "\n",
    "\n",
    "    pbar = tqdm(enumerate(loader), total=len(loader), desc=f\"[Train Epoch {epoch}]\")\n",
    "\n",
    "    for num_batch, (input_train, target_train) in pbar:\n",
    "        n = input_train.size(0)\n",
    "\n",
    "        input_train = input_train.to(device)\n",
    "        target_train = target_train.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, energy = model(input_train)\n",
    "        loss_task = criterion(logits, target_train)\n",
    "        loss_task.backward()\n",
    "        optimizer.step()\n",
    "        prec1, prec5 = utils.accuracy(logits, target_train, topk=(1, 5))\n",
    "        objs.update(loss_task.item(), n)\n",
    "        top1.update(prec1.item(), n)\n",
    "        top5.update(prec5.item(), n)\n",
    "        wandb.log({\n",
    "            \"train/step_loss\": objs.avg,\n",
    "            \"train/top_1\": top1.avg,\n",
    "            \"train/top_5\": top5.avg,\n",
    "            \"train/step_energy\": energy,\n",
    "            \"train/lr\": lr\n",
    "        })\n",
    "        pbar.set_postfix({\n",
    "            \"loss_task\": objs.avg,\n",
    "            \"accuracy\": top5.avg,\n",
    "            # \"f1_macro\": metrics_avg['f1_macro'].avg,\n",
    "            \"energy\": f\"{energy}\"\n",
    "        })\n",
    "\n",
    "    \n",
    "    return top1.avg, top5.avg, objs.avg\n",
    "\n",
    "def evaluate(loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    objs = utils.AvgrageMeter()\n",
    "    top1 = utils.AvgrageMeter()\n",
    "    top5 = utils.AvgrageMeter()\n",
    "    pbar = tqdm(loader, desc=\"[Valid]\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_valid, target_valid in pbar:\n",
    "            input_valid = input_valid.to(device)\n",
    "            target_valid = target_valid.to(device)\n",
    "            logits,_ = model(input_valid)\n",
    "            loss = criterion(logits, target_valid)\n",
    "            prec1, prec5 = utils.accuracy(logits, target_valid, topk=(1, 5))\n",
    "            n = input_valid.size(0)\n",
    "            objs.update(loss.item(), n)\n",
    "            top1.update(prec1.item(), n)\n",
    "            top5.update(prec5.item(), n)\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"val_loss\": objs.avg,\n",
    "                \"val_accuracy\": top1.avg,\n",
    "            })\n",
    "\n",
    "            \n",
    "    return top1.avg, top5.avg, objs.avg\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wandb.init(project=\"segmentation-darts\", name=args.save)\n",
    "    wandb.config.update(args)\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Nas",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
