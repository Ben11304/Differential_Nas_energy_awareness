{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca2b6957-eed9-4844-a3f7-f10952f0f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "def load_resnet_rgbn(model_path, architecture='resnet18'):\n",
    "    # Create model with four-channel input\n",
    "    model = getattr(models, architecture)(pretrained=False)\n",
    "    model.conv1 = torch.nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    model = torch.nn.Sequential(*list(model.children())[:-1], torch.nn.Flatten())\n",
    "    \n",
    "    # Load weights\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "model = load_resnet_rgbn('Res_18.pth', 'resnet18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49e9e3ad-80bb-4afc-9bc6-61f494ed87e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-28 11:34:11--  https://zenodo.org/record/8170135/files/Res_18.pth\n",
      "Resolving zenodo.org (zenodo.org)... 188.185.43.25, 188.185.48.194, 188.185.45.92, ...\n",
      "Connecting to zenodo.org (zenodo.org)|188.185.43.25|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
      "Location: /records/8170135/files/Res_18.pth [following]\n",
      "--2025-04-28 11:34:12--  https://zenodo.org/records/8170135/files/Res_18.pth\n",
      "Reusing existing connection to zenodo.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 44793425 (43M) [application/octet-stream]\n",
      "Saving to: ‘Res_18.pth’\n",
      "\n",
      "Res_18.pth          100%[===================>]  42.72M  5.53MB/s    in 9.9s    \n",
      "\n",
      "2025-04-28 11:34:22 (4.30 MB/s) - ‘Res_18.pth’ saved [44793425/44793425]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://zenodo.org/record/8170135/files/Res_18.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1665dcac-99ff-4ba5-9ab4-3a4e6331dd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_modified_miou(preds, targets, num_classes=9):\n",
    "    \"\"\"\n",
    "    preds, targets: Tensor [C, H, W] hoặc [B, C, H, W]\n",
    "    \"\"\"\n",
    "    if preds.dim() == 4:\n",
    "        preds = preds.flatten(0, 1)\n",
    "        targets = targets.flatten(0, 1)\n",
    "\n",
    "    C, H, W = preds.shape\n",
    "\n",
    "    # preds, targets: [C, H, W] --> [H, W, C]\n",
    "    preds = preds.permute(1, 2, 0)\n",
    "    targets = targets.permute(1, 2, 0)\n",
    "\n",
    "    # Boolean masks\n",
    "    preds_bool = (torch.sigmoid(preds) > 0.5)\n",
    "\n",
    "    targets_bool = targets > 0\n",
    "\n",
    "    # True Positive: pred đúng ít nhất 1 nhãn\n",
    "    correct = (preds_bool & targets_bool).float()\n",
    "\n",
    "    # False Positive: pred có nhãn sai\n",
    "    pred_only = preds_bool & (~targets_bool)\n",
    "    # False Negative: thiếu nhãn\n",
    "    target_only = targets_bool & (~preds_bool)\n",
    "\n",
    "    TP = correct.sum(dim=(0, 1))\n",
    "    FP = pred_only.sum(dim=(0, 1))\n",
    "    FN = target_only.sum(dim=(0, 1))\n",
    "\n",
    "    ious = TP / (TP + FP + FN + 1e-6)\n",
    "    miou = ious.mean().item()\n",
    "\n",
    "    return miou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cac18d4-aea1-4654-a254-b37330942ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "import logging\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.datasets as dset\n",
    "import torch.backends.cudnn as cudnn\n",
    "from model import NetworkCIFAR as Network\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "class CustomPTSegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        super().__init__()\n",
    "        self.images_dir = os.path.join(root_dir, \"X\")\n",
    "        self.labels_dir = os.path.join(root_dir, \"labels\")\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(self.images_dir, '*.pt')))\n",
    "        self.label_paths = [os.path.join(self.labels_dir, os.path.basename(p)) for p in self.image_paths]\n",
    "        assert all([os.path.exists(p) for p in self.label_paths]), \"Missing label files.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.load(self.image_paths[idx]).float().div(255.0)  # Normalize\n",
    "        label = torch.load(self.label_paths[idx]).float()\n",
    "        image = image.permute(0, 2, 1)  # [C, H, W]\n",
    "        label = label.permute(0, 2, 1)\n",
    "        if self.transform:\n",
    "            image, label = self.transform(image, label)\n",
    "        return image, label\n",
    "full_dataset = CustomPTSegmentationDataset(root_dir=\"../../../Agriculture-Vision-2021_processed_zip/trainrandcrop256/\")\n",
    "indices = list(range(5000))\n",
    "np.random.shuffle(indices)\n",
    "train_indices = indices[:2500]\n",
    "val_indices = indices[2500:5000]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(full_dataset, batch_size=16,\n",
    "    sampler=torch.utils.data.SubsetRandomSampler(train_indices), num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(full_dataset, batch_size=16,\n",
    "sampler=torch.utils.data.SubsetRandomSampler(val_indices), num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cfe3b81-c7d5-47d1-83cb-c9bbd696218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, device, num_classes=9):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    total_miou = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    pbar = tqdm(val_loader, desc=\"Evaluating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Dự đoán\n",
    "            logits = model(images)\n",
    "\n",
    "            # logits đầu ra [B, 512], cần reshape lại [B, num_classes, H, W]\n",
    "            # Tuy nhiên mô hình của bạn hiện tại đang flatten hết rồi.\n",
    "            # -> Nếu model output không phải [B, 9, H, W], bạn phải kiểm tra lại nhé.\n",
    "\n",
    "            if logits.dim() == 2:  # (batch_size, feature_dim)\n",
    "                # (Vì model Sequential cắt ở Flatten, output là 512-dim vector)\n",
    "                raise ValueError(\"Model output is flatten vector! Bạn cần chỉnh lại model để output ra mask segmentation.\")\n",
    "            \n",
    "            # Tính mIoU\n",
    "            batch_miou = compute_modified_miou(logits, labels, num_classes=num_classes)\n",
    "            batch_size = images.size(0)\n",
    "\n",
    "            total_miou += batch_miou * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "            pbar.set_postfix({\"Batch mIoU\": batch_miou})\n",
    "\n",
    "    avg_miou = total_miou / total_samples\n",
    "    return avg_miou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1872a64-ae41-4451-82a2-f1e864a054cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_flatten_model(model, val_loader, device, num_classes=9, threshold=0.5):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    total_correct = 0\n",
    "    total_labels = 0\n",
    "\n",
    "    pbar = tqdm(val_loader, desc=\"Evaluating (Flatten)\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Predict\n",
    "            logits = model(images)  # [B, 512]\n",
    "            preds = torch.sigmoid(logits)\n",
    "\n",
    "            # Reduce labels [B, 9, H, W] -> [B, 9]\n",
    "            labels_reduced = (labels > 0).float().view(labels.size(0), labels.size(1), -1).max(dim=2)[0]  # max pooling theo H*W\n",
    "\n",
    "            # Predict 9 classes bằng 1 Linear\n",
    "            preds = preds[:, :num_classes]  # Giả sử chỉ lấy 9 lớp đầu tiên\n",
    "\n",
    "            # Threshold predictions\n",
    "            preds_binary = (preds > threshold).float()\n",
    "\n",
    "            # Calculate correct prediction\n",
    "            correct = (preds_binary == labels_reduced).float().sum()\n",
    "\n",
    "            total_correct += correct.item()\n",
    "            total_labels += labels_reduced.numel()\n",
    "\n",
    "            pbar.set_postfix({\"Acc\": total_correct / total_labels})\n",
    "\n",
    "    acc = total_correct / total_labels\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3fc0f03-d5c1-4502-a618-74717f086f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.43862222222222225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = load_resnet_rgbn('Res_18.pth', 'resnet18').to(device)\n",
    "\n",
    "acc = evaluate_flatten_model(model, val_loader, device, num_classes=9)\n",
    "print(\"Validation accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d777c72e-ad5e-4204-a0d0-2548d8b04c25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Nas",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
