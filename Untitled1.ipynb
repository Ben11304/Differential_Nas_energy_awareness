{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bb65be8-9dd3-4f08-8306-f7731dea3928",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.data_utils'; 'utils' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      6\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dl-energy-estimator\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m preprocess_and_normalize_energy_data, parse_codecarbon_output\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiments_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m split_data_set, fit_model, compute_log_transformed_features, apply_data_transforms, test_model\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Tạo một sample để dự đoán\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.data_utils'; 'utils' is not a package"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append('./dl-energy-estimator')\n",
    "from utils.data_utils import preprocess_and_normalize_energy_data, parse_codecarbon_output\n",
    "from utils.experiments_utils import split_data_set, fit_model, compute_log_transformed_features, apply_data_transforms, test_model\n",
    "# Tạo một sample để dự đoán\n",
    "sample = pd.DataFrame([{\n",
    "    \"batch_size\": 123,\n",
    "    \"image_size\": 127,\n",
    "    \"kernel_size\": 7,\n",
    "    \"input_size\":4,\n",
    "    \"output_size\":2,\n",
    "    \"in_channels\": 80,\n",
    "    \"out_channels\": 29,\n",
    "    \"stride\": 1,\n",
    "    \"padding\": 2,\n",
    "    \"attributed\": \"conv2d\",\n",
    "    \"sub_attributed\": \"tanh\",\n",
    "    \"macs\":2.18534484e+11\n",
    "}])\n",
    "\n",
    "\n",
    "op_name=sample[\"attributed\"][0]\n",
    "if op_name==\"activation\":\n",
    "    param_cols = ['batch_size','input_size']\n",
    "    sample_norm=sample[param_cols]\n",
    "    data_linear_with_log, param_cols_with_log = sample_norm, param_cols\n",
    "    sub=sample[\"sub_attributed\"][0]\n",
    "    with open(f\"./energy_model/{op_name}/{sub}/linear_test_conv_model.pkl\", \"rb\") as f:\n",
    "        loaded_model = pickle.load(f)\n",
    "    # Load transformers\n",
    "    with open(f\"./energy_model/{op_name}/{sub}/x_transformer.pkl\", \"rb\") as f:\n",
    "        x_transformer = pickle.load(f)\n",
    "    \n",
    "    with open(f\"./energy_model/{op_name}/{sub}/y_transformer.pkl\", \"rb\") as f:\n",
    "        y_transformer = pickle.load(f)\n",
    "    \n",
    "    X_new_transformed = x_transformer.transform(data_linear_with_log)\n",
    "\n",
    "    # Dự đoán và inverse scale (nếu muốn)\n",
    "    y_pred_scaled = loaded_model.predict(X_new_transformed)\n",
    "    y_pred = y_transformer.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
    "    \n",
    "    print(f\"📌 Dự đoán năng lượng tiêu thụ: {y_pred[0][0]}\")\n",
    "\n",
    "else:\n",
    "    if op_name==\"linear\":\n",
    "        param_cols = ['batch_size','input_size','output_size']\n",
    "    if op_name==\"maxpool2d\":\n",
    "        param_cols=['batch_size', 'image_size', 'kernel_size', 'in_channels', 'stride', 'padding']\n",
    "    if op_name==\"conv2d\":\n",
    "        param_cols = ['batch_size','image_size','kernel_size','in_channels','out_channels','stride','padding']\n",
    "        \n",
    "    sample_norm=sample[param_cols]\n",
    "    data_linear_with_log, param_cols_with_log = compute_log_transformed_features(sample_norm, param_cols)\n",
    "    data_linear_with_log['macs']=sample[\"macs\"]\n",
    "    \n",
    "    with open(f\"./energy_model/{op_name}/linear_test_conv_model.pkl\", \"rb\") as f:\n",
    "        loaded_model = pickle.load(f)\n",
    "    # Load transformers\n",
    "    with open(f\"./energy_model/{op_name}/x_transformer.pkl\", \"rb\") as f:\n",
    "        x_transformer = pickle.load(f)\n",
    "    \n",
    "    with open(f\"./energy_model/{op_name}/y_transformer.pkl\", \"rb\") as f:\n",
    "        y_transformer = pickle.load(f)\n",
    "    \n",
    "    X_new_transformed = x_transformer.transform(data_linear_with_log)\n",
    "    \n",
    "    # Dự đoán và inverse scale (nếu muốn)\n",
    "    y_pred_scaled = loaded_model.predict(X_new_transformed)\n",
    "    y_pred = y_transformer.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
    "    \n",
    "    print(f\"📌 Dự đoán năng lượng tiêu thụ: {y_pred[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb8eedf-899c-4b2e-bfbf-9bf46bbe6f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "import logging\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.datasets as dset\n",
    "import torch.backends.cudnn as cudnn\n",
    "from model import NetworkCIFAR as Network\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "class CustomPTSegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        super().__init__()\n",
    "        self.images_dir = os.path.join(root_dir, \"X\")\n",
    "        self.labels_dir = os.path.join(root_dir, \"labels\")\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(self.images_dir, '*.pt')))\n",
    "        self.label_paths = [os.path.join(self.labels_dir, os.path.basename(p)) for p in self.image_paths]\n",
    "        assert all([os.path.exists(p) for p in self.label_paths]), \"Missing label files.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.load(self.image_paths[idx]).float().div(255.0)  # Normalize\n",
    "        label = torch.load(self.label_paths[idx]).float()\n",
    "        image = image.permute(0, 2, 1)  # [C, H, W]\n",
    "        label = label.permute(0, 2, 1)\n",
    "        if self.transform:\n",
    "            image, label = self.transform(image, label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a0c882f-0ec7-4248-8784-ed184fc181db",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = CustomPTSegmentationDataset(root_dir=\"../../../Agriculture-Vision-2021_processed_zip/trainrandcrop256/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "548a2dc6-b9b2-48da-8e5a-2182a4b696e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing statistics: 100%|██████████| 500/500 [00:02<00:00, 249.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average image height: 512.00 pixels\n",
      "Average image width: 512.00 pixels\n",
      "\n",
      "Class pixel distribution:\n",
      "Class 0: 1091411 pixels (0.83%)\n",
      "Class 1: 15994505 pixels (12.20%)\n",
      "Class 2: 1082773 pixels (0.83%)\n",
      "Class 3: 9624748 pixels (7.34%)\n",
      "Class 4: 130532 pixels (0.10%)\n",
      "Class 5: 1992060 pixels (1.52%)\n",
      "Class 6: 534739 pixels (0.41%)\n",
      "Class 7: 7907236 pixels (6.03%)\n",
      "Class 8: 92967389 pixels (70.93%)\n",
      "\n",
      "Total pixels counted: 131325393\n",
      "Expected total pixels: 131072000.0\n",
      "Warning: Pixel count mismatch, possible data inconsistency.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CustomPTSegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        super().__init__()\n",
    "        self.images_dir = os.path.join(root_dir, \"X\")\n",
    "        self.labels_dir = os.path.join(root_dir, \"labels\")\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(self.images_dir, '*.pt')))\n",
    "        self.label_paths = [os.path.join(self.labels_dir, os.path.basename(p)) for p in self.image_paths]\n",
    "        assert all([os.path.exists(p) for p in self.label_paths]), \"Missing label files.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.load(self.image_paths[idx]).float().div(255.0)  # Normalize\n",
    "        label = torch.load(self.label_paths[idx]).float()\n",
    "        image = image.permute(0, 2, 1)  # [C, H, W]\n",
    "        label = label.permute(0, 2, 1)\n",
    "        if self.transform:\n",
    "            image, label = self.transform(image, label)\n",
    "        return image, label\n",
    "\n",
    "def compute_statistics(dataset):\n",
    "    total_samples = len(dataset)\n",
    "    print(f\"Total number of samples: {total_samples}\")\n",
    "\n",
    "    # Khởi tạo biến thống kê\n",
    "    total_height = 0\n",
    "    total_width = 0\n",
    "    class_pixel_counts = torch.zeros(9, dtype=torch.long)  # 9 lớp: 0-8\n",
    "    total_pixels = 0\n",
    "\n",
    "    # Sử dụng tqdm để theo dõi tiến trình\n",
    "    with tqdm(total=total_samples, desc=\"Computing statistics\") as pbar:\n",
    "        for idx in range(total_samples):\n",
    "            try:\n",
    "                image, label = dataset[idx]\n",
    "\n",
    "                # Kích thước ảnh\n",
    "                _, H, W = image.shape\n",
    "                total_height += H\n",
    "                total_width += W\n",
    "\n",
    "                # Đếm pixel theo lớp\n",
    "                # label: [9, H, W], giá trị 0 hoặc 1 cho mỗi lớp\n",
    "                for class_idx in range(9):\n",
    "                    class_pixel_counts[class_idx] += torch.sum(label[class_idx] > 0).item()\n",
    "                total_pixels += H * W\n",
    "\n",
    "                pbar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Tính trung bình kích thước\n",
    "    avg_height = total_height / total_samples\n",
    "    avg_width = total_width / total_samples\n",
    "    print(f\"Average image height: {avg_height:.2f} pixels\")\n",
    "    print(f\"Average image width: {avg_width:.2f} pixels\")\n",
    "\n",
    "    # Tính tỷ lệ phân phối lớp\n",
    "    class_percentages = (class_pixel_counts.float() / total_pixels) * 100\n",
    "    print(\"\\nClass pixel distribution:\")\n",
    "    for class_idx in range(9):\n",
    "        print(f\"Class {class_idx}: {class_pixel_counts[class_idx]} pixels ({class_percentages[class_idx]:.2f}%)\")\n",
    "\n",
    "    # Kiểm tra tổng số pixel\n",
    "    print(f\"\\nTotal pixels counted: {class_pixel_counts.sum().item()}\")\n",
    "    print(f\"Expected total pixels: {total_samples * avg_height * avg_width}\")\n",
    "    if abs(class_pixel_counts.sum().item() - (total_samples * avg_height * avg_width)) > 1e-5:\n",
    "        print(\"Warning: Pixel count mismatch, possible data inconsistency.\")\n",
    "\n",
    "    # Kiểm tra giá trị bất thường\n",
    "    for idx in range(total_samples):\n",
    "        try:\n",
    "            _, label = dataset[idx]\n",
    "            if torch.any(torch.isnan(label)) or torch.any(label < 0) or torch.any(label > 1):\n",
    "                print(f\"Warning: Invalid values detected in label at index {idx}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking sample {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = \"../../../Agriculture-Vision-2021_processed_zip/trainrandcrop256/\"\n",
    "    dataset = CustomPTSegmentationDataset(root_dir)\n",
    "    compute_statistics(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae3c923-5a7f-400e-a36d-0a21e1eb9cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing statistics and labels: 100%|██████████| 5000/5000 [03:06<00:00, 26.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average image height: 256.00 pixels\n",
      "Average image width: 256.00 pixels\n",
      "\n",
      "Class pixel distribution:\n",
      "Double plant (Class 0): 2536671 pixels (0.77%)\n",
      "Drydown (Class 1): 41408797 pixels (12.64%)\n",
      "Endrow (Class 2): 3084536 pixels (0.94%)\n",
      "Nutrient deficiency (Class 3): 24665003 pixels (7.53%)\n",
      "Planter skip (Class 4): 653315 pixels (0.20%)\n",
      "Water (Class 5): 3476414 pixels (1.06%)\n",
      "Waterway (Class 6): 2160273 pixels (0.66%)\n",
      "Weed cluster (Class 7): 20049768 pixels (6.12%)\n",
      "background (Class 8): 230127092 pixels (70.23%)\n",
      "\n",
      "Total pixels counted: 328161869\n",
      "Expected total pixels: 327680000\n",
      "Warning: Pixel count mismatch, possible data inconsistency.\n",
      "\n",
      "Distribution of number of labels per image:\n",
      "Images with 1 label(s): 30 (0.60%)\n",
      "Images with 2 label(s): 4662 (93.24%)\n",
      "Images with 3 label(s): 273 (5.46%)\n",
      "Images with 4 label(s): 35 (0.70%)\n",
      "\n",
      "Sample label assignments (first 10 images):\n",
      "Image 0: ['Double plant', 'background'] (2 labels)\n",
      "Image 1: ['Double plant', 'background'] (2 labels)\n",
      "Image 2: ['Double plant', 'background'] (2 labels)\n",
      "Image 3: ['Double plant', 'background'] (2 labels)\n",
      "Image 4: ['Planter skip', 'background'] (2 labels)\n",
      "Image 5: ['Drydown', 'background'] (2 labels)\n",
      "Image 6: ['Drydown', 'Waterway', 'background'] (3 labels)\n",
      "Image 7: ['Endrow', 'background'] (2 labels)\n",
      "Image 8: ['Endrow', 'background'] (2 labels)\n",
      "Image 9: ['Drydown', 'background'] (2 labels)\n",
      "\n",
      "Checking for invalid label values...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class CustomPTSegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        super().__init__()\n",
    "        self.images_dir = os.path.join(root_dir, \"X\")\n",
    "        self.labels_dir = os.path.join(root_dir, \"labels\")\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(self.images_dir, '*.pt')))\n",
    "        self.label_paths = [os.path.join(self.labels_dir, os.path.basename(p)) for p in self.image_paths]\n",
    "        assert all([os.path.exists(p) for p in self.label_paths]), \"Missing label files.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.load(self.image_paths[idx]).float().div(255.0)  # Normalize\n",
    "        label = torch.load(self.label_paths[idx]).float()\n",
    "        image = image.permute(0, 2, 1)  # [C, H, W]\n",
    "        label = label.permute(0, 2, 1)  # [9, H, W]\n",
    "        if self.transform:\n",
    "            image, label = self.transform(image, label)\n",
    "        return image, label\n",
    "\n",
    "def compute_statistics_and_labels(dataset):\n",
    "    total_samples = len(dataset)\n",
    "    print(f\"Total number of samples: {total_samples}\")\n",
    "\n",
    "    # Khởi tạo biến thống kê\n",
    "    total_height = 0\n",
    "    total_width = 0\n",
    "    class_pixel_counts = torch.zeros(9, dtype=torch.long)  # 9 lớp: 0-8\n",
    "    total_pixels = 0\n",
    "    labels_per_image = []  # Lưu danh sách các lớp có trong mỗi ảnh\n",
    "    label_count_distribution = np.zeros(10, dtype=int)  # Đếm số ảnh có 0, 1, ..., 9 nhãn\n",
    "\n",
    "    # Tên lớp cho in ra kết quả\n",
    "    class_names = [\n",
    "        \"Double plant\", \"Drydown\", \"Endrow\", \"Nutrient deficiency\",\n",
    "        \"Planter skip\", \"Water\", \"Waterway\", \"Weed cluster\", \"background\"\n",
    "    ]\n",
    "\n",
    "    # Sử dụng tqdm để theo dõi tiến trình\n",
    "    with tqdm(total=total_samples, desc=\"Computing statistics and labels\") as pbar:\n",
    "        for idx in range(total_samples):\n",
    "            try:\n",
    "                image, label = dataset[idx]\n",
    "\n",
    "                # Kích thước ảnh\n",
    "                _, H, W = image.shape\n",
    "                total_height += H\n",
    "                total_width += W\n",
    "\n",
    "                # Đếm pixel theo lớp và xác định nhãn có trong ảnh\n",
    "                image_labels = []\n",
    "                for class_idx in range(9):\n",
    "                    pixel_count = torch.sum(label[class_idx] > 0).item()\n",
    "                    class_pixel_counts[class_idx] += pixel_count\n",
    "                    if pixel_count > 0:\n",
    "                        image_labels.append(class_idx)\n",
    "                total_pixels += H * W\n",
    "\n",
    "                # Lưu danh sách nhãn của ảnh này\n",
    "                labels_per_image.append(image_labels)\n",
    "                # Cập nhật phân phối số lượng nhãn\n",
    "                num_labels = len(image_labels)\n",
    "                label_count_distribution[num_labels] += 1\n",
    "\n",
    "                pbar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Tính trung bình kích thước\n",
    "    avg_height = total_height / total_samples\n",
    "    avg_width = total_width / total_samples\n",
    "    print(f\"\\nAverage image height: {avg_height:.2f} pixels\")\n",
    "    print(f\"Average image width: {avg_width:.2f} pixels\")\n",
    "\n",
    "    # Tính tỷ lệ phân phối lớp\n",
    "    class_percentages = (class_pixel_counts.float() / total_pixels) * 100\n",
    "    print(\"\\nClass pixel distribution:\")\n",
    "    for class_idx in range(9):\n",
    "        print(f\"{class_names[class_idx]} (Class {class_idx}): {class_pixel_counts[class_idx]} pixels ({class_percentages[class_idx]:.2f}%)\")\n",
    "\n",
    "    # Kiểm tra tổng số pixel\n",
    "    print(f\"\\nTotal pixels counted: {class_pixel_counts.sum().item()}\")\n",
    "    print(f\"Expected total pixels: {total_samples * avg_height * avg_width:.0f}\")\n",
    "    if abs(class_pixel_counts.sum().item() - (total_samples * avg_height * avg_width)) > 1e-5:\n",
    "        print(\"Warning: Pixel count mismatch, possible data inconsistency.\")\n",
    "\n",
    "    # Thống kê phân phối số lượng nhãn mỗi ảnh\n",
    "    print(\"\\nDistribution of number of labels per image:\")\n",
    "    for num_labels in range(10):\n",
    "        if label_count_distribution[num_labels] > 0:\n",
    "            print(f\"Images with {num_labels} label(s): {label_count_distribution[num_labels]} ({label_count_distribution[num_labels]/total_samples*100:.2f}%)\")\n",
    "\n",
    "    # In danh sách nhãn cho một số ảnh mẫu (ví dụ: 10 ảnh đầu)\n",
    "    print(\"\\nSample label assignments (first 10 images):\")\n",
    "    for idx in range(min(10, total_samples)):\n",
    "        if labels_per_image[idx]:\n",
    "            label_names = [class_names[i] for i in labels_per_image[idx]]\n",
    "            print(f\"Image {idx}: {label_names} ({len(label_names)} labels)\")\n",
    "        else:\n",
    "            print(f\"Image {idx}: No labels\")\n",
    "\n",
    "    # Kiểm tra giá trị bất thường\n",
    "    print(\"\\nChecking for invalid label values...\")\n",
    "    for idx in range(total_samples):\n",
    "        try:\n",
    "            _, label = dataset[idx]\n",
    "            if torch.any(torch.isnan(label)) or torch.any(label < 0) or torch.any(label > 1):\n",
    "                print(f\"Warning: Invalid values detected in label at index {idx}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking sample {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = \"../../../Agriculture-Vision-2021_processed_zip/trainrandcrop256/\"\n",
    "    dataset = CustomPTSegmentationDataset(root_dir)\n",
    "    compute_statistics_and_labels(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b659dd7-389d-4559-9a94-346a8851e1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing statistics and labels: 100%|██████████| 30000/30000 [00:29<00:00, 1030.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average image height: 64.00 pixels\n",
      "Average image width: 64.00 pixels\n",
      "\n",
      "Class pixel distribution:\n",
      "Double plant (Class 0): 1107579 pixels (0.90%)\n",
      "Drydown (Class 1): 17110534 pixels (13.92%)\n",
      "Endrow (Class 2): 1192208 pixels (0.97%)\n",
      "Nutrient deficiency (Class 3): 9632847 pixels (7.84%)\n",
      "Planter skip (Class 4): 265605 pixels (0.22%)\n",
      "Water (Class 5): 1386593 pixels (1.13%)\n",
      "Waterway (Class 6): 1160899 pixels (0.94%)\n",
      "Weed cluster (Class 7): 8224195 pixels (6.69%)\n",
      "\n",
      "Number of images containing each class:\n",
      "Double plant (Class 0): 678 images (2.26%)\n",
      "Drydown (Class 1): 5368 images (17.89%)\n",
      "Endrow (Class 2): 737 images (2.46%)\n",
      "Nutrient deficiency (Class 3): 3252 images (10.84%)\n",
      "Planter skip (Class 4): 198 images (0.66%)\n",
      "Water (Class 5): 504 images (1.68%)\n",
      "Waterway (Class 6): 546 images (1.82%)\n",
      "Weed cluster (Class 7): 2989 images (9.96%)\n",
      "\n",
      "Total pixels counted: 40080460\n",
      "Expected total pixels: 122880000\n",
      "Warning: Pixel count mismatch, possible data inconsistency.\n",
      "\n",
      "Most common label combinations (top 5):\n",
      "No labels: 15871 images (52.90%)\n",
      "Combination ['Drydown']: 5363 images (17.88%)\n",
      "Combination ['Nutrient deficiency']: 3250 images (10.83%)\n",
      "Combination ['Weed cluster']: 2900 images (9.67%)\n",
      "Combination ['Endrow']: 648 images (2.16%)\n",
      "\n",
      "Checking for invalid label values...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class CustomPTSegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        super().__init__()\n",
    "        self.images_dir = os.path.join(root_dir, \"X\")\n",
    "        self.labels_dir = os.path.join(root_dir, \"labels\")\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(self.images_dir, '*.pt')))\n",
    "        self.label_paths = [os.path.join(self.labels_dir, os.path.basename(p)) for p in self.image_paths]\n",
    "        assert all([os.path.exists(p) for p in self.label_paths]), \"Missing label files.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.load(self.image_paths[idx]).float().div(255.0)  # Normalize\n",
    "        label = torch.load(self.label_paths[idx]).float()\n",
    "        image = image.permute(0, 2, 1)  # [C, H, W]\n",
    "        label = label.permute(0, 2, 1)  # [9, H, W]\n",
    "        if self.transform:\n",
    "            image, label = self.transform(image, label)\n",
    "        return image, label\n",
    "\n",
    "def compute_statistics_and_labels(dataset):\n",
    "    total_samples = len(dataset)\n",
    "    print(f\"Total number of samples: {total_samples}\")\n",
    "\n",
    "    # Khởi tạo biến thống kê\n",
    "    total_height = 0\n",
    "    total_width = 0\n",
    "    class_pixel_counts = torch.zeros(8, dtype=torch.long)  # Số pixel mỗi lớp\n",
    "    class_image_counts = torch.zeros(8, dtype=torch.long)  # Số ảnh chứa mỗi lớp\n",
    "    total_pixels = 0\n",
    "    label_combinations = []  # Lưu các bộ nhãn để phân tích kết hợp\n",
    "\n",
    "    # Tên lớp\n",
    "    class_names = [\n",
    "        \"Double plant\", \"Drydown\", \"Endrow\", \"Nutrient deficiency\",\n",
    "        \"Planter skip\", \"Water\", \"Waterway\", \"Weed cluster\"\n",
    "    ]\n",
    "\n",
    "    # Sử dụng tqdm để theo dõi tiến trình\n",
    "    with tqdm(total=total_samples, desc=\"Computing statistics and labels\") as pbar:\n",
    "        for idx in range(total_samples):\n",
    "            try:\n",
    "                image, label = dataset[idx]\n",
    "\n",
    "                # Kích thước ảnh\n",
    "                _, H, W = image.shape\n",
    "                total_height += H\n",
    "                total_width += W\n",
    "\n",
    "                # Đếm pixel và kiểm tra sự hiện diện của lớp\n",
    "                image_labels = []\n",
    "                for class_idx in range(8):\n",
    "                    pixel_count = torch.sum(label[class_idx] > 0).item()\n",
    "                    class_pixel_counts[class_idx] += pixel_count\n",
    "                    if pixel_count > 0:\n",
    "                        class_image_counts[class_idx] += 1\n",
    "                        image_labels.append(class_idx)\n",
    "                total_pixels += H * W\n",
    "\n",
    "                # Lưu bộ nhãn cho phân tích kết hợp\n",
    "                label_combinations.append(tuple(sorted(image_labels)))\n",
    "\n",
    "                pbar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Tính trung bình kích thước\n",
    "    avg_height = total_height / total_samples\n",
    "    avg_width = total_width / total_samples\n",
    "    print(f\"\\nAverage image height: {avg_height:.2f} pixels\")\n",
    "    print(f\"Average image width: {avg_width:.2f} pixels\")\n",
    "\n",
    "    # Tính tỷ lệ phân phối pixel theo lớp\n",
    "    class_pixel_percentages = (class_pixel_counts.float() / total_pixels) * 100\n",
    "    print(\"\\nClass pixel distribution:\")\n",
    "    for class_idx in range(8):\n",
    "        print(f\"{class_names[class_idx]} (Class {class_idx}): {class_pixel_counts[class_idx]} pixels ({class_pixel_percentages[class_idx]:.2f}%)\")\n",
    "\n",
    "    # Thống kê số ảnh chứa mỗi lớp\n",
    "    class_image_percentages = (class_image_counts.float() / total_samples) * 100\n",
    "    print(\"\\nNumber of images containing each class:\")\n",
    "    for class_idx in range(8):\n",
    "        print(f\"{class_names[class_idx]} (Class {class_idx}): {class_image_counts[class_idx]} images ({class_image_percentages[class_idx]:.2f}%)\")\n",
    "\n",
    "    # Kiểm tra tổng số pixel\n",
    "    print(f\"\\nTotal pixels counted: {class_pixel_counts.sum().item()}\")\n",
    "    print(f\"Expected total pixels: {total_samples * avg_height * avg_width:.0f}\")\n",
    "    if abs(class_pixel_counts.sum().item() - (total_samples * avg_height * avg_width)) > 1e-5:\n",
    "        print(\"Warning: Pixel count mismatch, possible data inconsistency.\")\n",
    "\n",
    "    # (Tùy chọn) Thống kê các kết hợp nhãn phổ biến\n",
    "    print(\"\\nMost common label combinations (top 5):\")\n",
    "    combination_counts = Counter(label_combinations)\n",
    "    for comb, count in combination_counts.most_common(5):\n",
    "        if comb:\n",
    "            comb_names = [class_names[i] for i in comb]\n",
    "            print(f\"Combination {comb_names}: {count} images ({count/total_samples*100:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"No labels: {count} images ({count/total_samples*100:.2f}%)\")\n",
    "\n",
    "    # Kiểm tra giá trị bất thường\n",
    "    print(\"\\nChecking for invalid label values...\")\n",
    "    for idx in range(total_samples):\n",
    "        try:\n",
    "            _, label = dataset[idx]\n",
    "            if torch.any(torch.isnan(label)) or torch.any(label < 0) or torch.any(label > 1):\n",
    "                print(f\"Warning: Invalid values detected in label at index {idx}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking sample {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = \"../../../Agriculture-Vision-2021_processed_zip/trainrandcrop256/\"\n",
    "    dataset = CustomPTSegmentationDataset(root_dir)\n",
    "    compute_statistics_and_labels(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2e9a56-bae4-4d21-98c3-109ad1ada322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train:  38%|███▊      | 11281/30000 [31:06<35:36,  8.76it/s]  IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing train:  61%|██████    | 18327/30000 [53:14<46:02,  4.23it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "import logging\n",
    "\n",
    "# Cấu hình logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def is_binary_image(image_array):\n",
    "    \"\"\"Kiểm tra xem ảnh có phải nhị phân (chỉ chứa 0 và/hoặc 1) không.\"\"\"\n",
    "    unique_values = np.unique(image_array)\n",
    "    return set(unique_values).issubset({0, 1})\n",
    "\n",
    "def normalize_if_not_binary(image_array):\n",
    "    \"\"\"Chuẩn hóa ảnh về [0, 1] nếu không phải nhị phân.\"\"\"\n",
    "    if not is_binary_image(image_array):\n",
    "        return image_array / 255.0\n",
    "    return image_array.astype(np.float32) / 255.0  # Chuyển về [0, 1] ngay cả khi nhị phân\n",
    "\n",
    "def find_image_path(root_dir, image_name_base):\n",
    "    exts = ['png', 'jpg', 'jpeg']\n",
    "    for ext in exts:\n",
    "        path = os.path.join(root_dir, f\"{image_name_base}.{ext}\")\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "    raise FileNotFoundError(f\"File {image_name_base}.[png/jpg/jpeg] not found in {root_dir}\")\n",
    "\n",
    "def random_crop(img_tensor, label_tensor, region_mask_tensor, crop_size=(256, 256)):\n",
    "    _, H, W = img_tensor.shape\n",
    "    ch, cw = crop_size\n",
    "\n",
    "    valid_positions = (region_mask_tensor > 0).nonzero(as_tuple=False)\n",
    "\n",
    "    if len(valid_positions) == 0:\n",
    "        top = (H - ch) // 2\n",
    "        left = (W - cw) // 2\n",
    "    else:\n",
    "        idx = random.randint(0, len(valid_positions) - 1)\n",
    "        center_y, center_x = valid_positions[idx, 0].item(), valid_positions[idx, 1].item()\n",
    "        top = max(0, min(H - ch, center_y - ch // 2))\n",
    "        left = max(0, min(W - cw, center_x - cw // 2))\n",
    "\n",
    "    img_cropped = TF.crop(img_tensor, top, left, ch, cw)\n",
    "    label_cropped = TF.crop(label_tensor, top, left, ch, cw)\n",
    "    mask_cropped = TF.crop(region_mask_tensor.unsqueeze(0), top, left, ch, cw).squeeze(0)\n",
    "\n",
    "    img_cropped = img_cropped * mask_cropped.unsqueeze(0)\n",
    "    label_cropped = label_cropped * mask_cropped.unsqueeze(0)\n",
    "\n",
    "    return img_cropped, label_cropped\n",
    "\n",
    "def load_sample(image_name, image_dir, mask_dir, boundary_dir, label_root, label_class_names, crop_size=(256, 256)):\n",
    "    image_name_base = os.path.splitext(image_name)[0]\n",
    "\n",
    "    try:\n",
    "        # Load RGB\n",
    "        image_path = find_image_path(image_dir, image_name_base)\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        img_array = np.array(img)\n",
    "        img_4ch = normalize_if_not_binary(img_array)  # Chuẩn hóa RGB nếu cần\n",
    "\n",
    "        # Load NIR\n",
    "        nir_path = image_path.replace('/rgb/', '/nir/')\n",
    "        nir_path = find_image_path(os.path.dirname(nir_path), image_name_base)\n",
    "        nir = Image.open(nir_path).convert('L')\n",
    "        nir_array = np.array(nir)\n",
    "        nir_normalized = normalize_if_not_binary(nir_array)  # Chuẩn hóa NIR nếu cần\n",
    "        img_4ch = np.concatenate([img_4ch, nir_normalized[:, :, None]], axis=-1)\n",
    "\n",
    "        # Load boundary and mask\n",
    "        boundary_path = find_image_path(boundary_dir, image_name_base)\n",
    "        mask_path = find_image_path(mask_dir, image_name_base)\n",
    "        boundary = np.array(Image.open(boundary_path)) > 0\n",
    "        valid_mask = np.array(Image.open(mask_path)) > 0\n",
    "        region_mask = boundary & valid_mask\n",
    "\n",
    "        # Load each class label\n",
    "        label_stack = []\n",
    "        for class_name in label_class_names:\n",
    "            label_class_path = os.path.join(label_root, class_name)\n",
    "            label_path = find_image_path(label_class_path, image_name_base)\n",
    "            label = np.array(Image.open(label_path).convert('L'))  # Đảm bảo là ảnh xám\n",
    "            label_normalized = normalize_if_not_binary(label)\n",
    "            label_stack.append(label_normalized)\n",
    "\n",
    "        label_stack = np.stack(label_stack, axis=-1)\n",
    "\n",
    "        # Convert to tensors\n",
    "        img_tensor = torch.from_numpy(img_4ch).permute(2, 0, 1).float()  # [4, H, W], float32\n",
    "        label_tensor = torch.from_numpy(label_stack).permute(2, 0, 1).float()  # [num_classes, H, W]\n",
    "        region_mask_tensor = torch.from_numpy(region_mask.astype(np.uint8))  # [H, W]\n",
    "\n",
    "        # Random crop\n",
    "        img_tensor, label_tensor = random_crop(img_tensor, label_tensor, region_mask_tensor, crop_size=crop_size)\n",
    "        # print(np.shape(label_tensor))\n",
    "        # print(np.shape(img_tensor))\n",
    "        # Add background\n",
    "        # background = (label_tensor.sum(dim=0, keepdim=True) < 1e-6).float()  # Sum over classes, keep shape [1, H, W]\n",
    "        # gt_binary = torch.cat([label_tensor, background], dim=0)  # Shape: [num_classes + 1, H, W]\n",
    "        # if torch.any(gt_binary.sum(dim=0) < 1e-6):  # Check per pixel\n",
    "        #     print(\"Some pixels in gt_binary have no labels, even after adding background\")\n",
    "\n",
    "        return img_tensor, label_tensor\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading sample {image_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "def process_dataset_folder(\n",
    "    base_dir,\n",
    "    save_dir,\n",
    "    save_format=\"pt\",  # or \"npy\"\n",
    "    crop_size=(256, 256),\n",
    "    max_samples=500  # Giới hạn số lượng sample\n",
    "):\n",
    "    img_dir = os.path.join(base_dir, \"images/rgb\")\n",
    "    mask_dir = os.path.join(base_dir, \"masks\")\n",
    "    boundary_dir = os.path.join(base_dir, \"boundaries\")\n",
    "    label_root = os.path.join(base_dir, \"labels\")\n",
    "\n",
    "    label_class_names = sorted([\n",
    "        d for d in os.listdir(label_root)\n",
    "        if os.path.isdir(os.path.join(label_root, d))\n",
    "    ])\n",
    "\n",
    "    os.makedirs(os.path.join(save_dir, \"X\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir, \"labels\"), exist_ok=True)\n",
    "\n",
    "    image_files = [f for f in os.listdir(img_dir) if f.endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "    image_files = image_files[:min(max_samples, len(image_files))]  # Giới hạn số sample\n",
    "\n",
    "    for image_name in tqdm(image_files, desc=f\"Processing {os.path.basename(base_dir)}\"):\n",
    "        try:\n",
    "            img_tensor, gt_binary = load_sample(\n",
    "                image_name, img_dir, mask_dir, boundary_dir, label_root, label_class_names, crop_size=crop_size\n",
    "            )\n",
    "            name_base = os.path.splitext(image_name)[0]\n",
    "            if save_format == \"pt\":\n",
    "                torch.save(img_tensor, os.path.join(save_dir, \"X\", f\"{name_base}.pt\"))\n",
    "                torch.save(gt_binary, os.path.join(save_dir, \"labels\", f\"{name_base}.pt\"))\n",
    "            elif save_format == \"npy\":\n",
    "                np.save(os.path.join(save_dir, \"X\", f\"{name_base}.npy\"), img_tensor.numpy())\n",
    "                np.save(os.path.join(save_dir, \"labels\", f\"{name_base}.npy\"), gt_binary.numpy())\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error processing {image_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"../../../Agriculture-Vision-2021/train\"\n",
    "    save_dir = \"../../../Agriculture-Vision-2021_processed_zip/trainrandcrop256/\"\n",
    "    process_dataset_folder(base_dir, save_dir, save_format=\"pt\", crop_size=(64, 64), max_samples=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c61288c-675e-45af-8072-2e4f823845e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting labels to vectors: 100%|██████████| 30000/30000 [00:32<00:00, 924.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 30000 files successfully.\n",
      "Saved vector labels to: ../../../Agriculture-Vision-2021_processed_zip/trainrandcrop256/vector_label\n",
      "Number of vector label files saved: 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CustomPTSegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        super().__init__()\n",
    "        self.images_dir = os.path.join(root_dir, \"X\")\n",
    "        self.labels_dir = os.path.join(root_dir, \"labels\")\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(self.images_dir, '*.pt')))\n",
    "        self.label_paths = [os.path.join(self.labels_dir, os.path.basename(p)) for p in self.image_paths]\n",
    "        assert all([os.path.exists(p) for p in self.label_paths]), \"Missing label files.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.load(self.image_paths[idx]).float().div(255.0)  # Normalize\n",
    "        label = torch.load(self.label_paths[idx]).float()\n",
    "        image = image.permute(0, 2, 1)  # [C, H, W]\n",
    "        label = label.permute(0, 2, 1)  # [9, H, W]\n",
    "        if self.transform:\n",
    "            image, label = self.transform(image, label)\n",
    "        return image, label\n",
    "\n",
    "def convert_labels_to_vector(root_dir):\n",
    "    # Tạo thư mục vector_label trong root_dir\n",
    "    vector_label_dir = os.path.join(root_dir, \"vector_label\")\n",
    "    os.makedirs(vector_label_dir, exist_ok=True)\n",
    "\n",
    "    # Khởi tạo dataset\n",
    "    dataset = CustomPTSegmentationDataset(root_dir)\n",
    "    total_samples = len(dataset)\n",
    "    print(f\"Total number of samples: {total_samples}\")\n",
    "\n",
    "    processed_files = 0\n",
    "    errors = 0\n",
    "\n",
    "    # Sử dụng tqdm để theo dõi tiến trình\n",
    "    with tqdm(total=total_samples, desc=\"Converting labels to vectors\") as pbar:\n",
    "        for idx in range(total_samples):\n",
    "            try:\n",
    "                # Chỉ cần nhãn\n",
    "                _, label = dataset[idx]  # label: [8, H, W]\n",
    "\n",
    "                # Chuyển tensor nhãn thành vector nhãn\n",
    "                vector_label = torch.zeros(8, dtype=torch.float)\n",
    "                for class_idx in range(8):\n",
    "                    if torch.sum(label[class_idx] > 0).item() > 0:\n",
    "                        vector_label[class_idx] = 1\n",
    "\n",
    "                # Lưu vector nhãn vào file .pt\n",
    "                output_path = os.path.join(vector_label_dir, os.path.basename(dataset.label_paths[idx]))\n",
    "                torch.save(vector_label, output_path)\n",
    "\n",
    "                processed_files += 1\n",
    "                pbar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {idx} ({dataset.label_paths[idx]}): {e}\")\n",
    "                errors += 1\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "    # Báo cáo kết quả\n",
    "    print(f\"\\nProcessed {processed_files} files successfully.\")\n",
    "    if errors > 0:\n",
    "        print(f\"Encountered {errors} errors.\")\n",
    "    print(f\"Saved vector labels to: {vector_label_dir}\")\n",
    "\n",
    "    # Kiểm tra số file lưu được\n",
    "    saved_files = len(glob.glob(os.path.join(vector_label_dir, '*.pt')))\n",
    "    print(f\"Number of vector label files saved: {saved_files}\")\n",
    "    if saved_files != processed_files:\n",
    "        print(\"Warning: Number of saved files does not match processed files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = \"../../../Agriculture-Vision-2021_processed_zip/trainrandcrop256/\"\n",
    "    convert_labels_to_vector(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a6c4fb-f77e-45f1-810b-354b8a38e925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/12000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 131072]' is invalid for input of size 16384",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 151\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 143\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m--> 143\u001b[0m     train_loss, train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     val_loss, val_metrics \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, device)\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 78\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     76\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Chuyển dữ liệu lên GPU/CPU\u001b[39;00m\n\u001b[1;32m     77\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 78\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, y)\n\u001b[1;32m     80\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Nas/darts_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Nas/darts_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m, in \u001b[0;36mSimpleCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[0;32m---> 25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Flatten\u001b[39;00m\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[1;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)  \u001b[38;5;66;03m# Logits đầu ra\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 131072]' is invalid for input of size 16384"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Định nghĩa mô hình CNN đơn giản\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        # Giả sử kích thước ảnh đầu vào là 256x256\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, 128)  # Sau 2 lần pooling: 256 -> 128 -> 64\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 64 * 64)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # Logits đầu ra\n",
    "        return x\n",
    "\n",
    "# Định nghĩa dataset (tái sử dụng từ code của bạn)\n",
    "class CustomPTSegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        super().__init__()\n",
    "        self.images_dir = os.path.join(root_dir, \"X\")\n",
    "        self.labels_dir = os.path.join(root_dir, \"vector_label\")\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(self.images_dir, '*.pt')))\n",
    "        self.label_paths = [os.path.join(self.labels_dir, os.path.basename(p)) for p in self.image_paths]\n",
    "        assert all([os.path.exists(p) for p in self.label_paths]), \"Missing label files.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.load(self.image_paths[idx]).float()\n",
    "        label = torch.load(self.label_paths[idx]).float()  # Giới hạn ở 8 lớp đầu tiên\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Hàm tính metrics\n",
    "def compute_metrics(preds, labels, threshold=0.5):\n",
    "    preds = torch.sigmoid(preds) > threshold\n",
    "    labels = labels > 0.5\n",
    "    tp = (preds & labels).sum(dim=0).float()\n",
    "    fp = (preds & ~labels).sum(dim=0).float()\n",
    "    fn = (~preds & labels).sum(dim=0).float()\n",
    "    precision = tp / (tp + fp + 1e-6)\n",
    "    recall = tp / (tp + fn + 1e-6)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "    accuracy = (preds == labels).float().mean(dim=0)\n",
    "    return {\n",
    "        'accuracy': accuracy.mean().item(),\n",
    "        'f1_macro': f1.mean().item()\n",
    "    }\n",
    "\n",
    "# Hàm huấn luyện\n",
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_metrics = {'accuracy': 0, 'f1_macro': 0}\n",
    "    total_samples = 0\n",
    "\n",
    "    for x, y in tqdm(loader, desc=\"Training\"):\n",
    "        x, y = x.to(device), y.to(device)  # Chuyển dữ liệu lên GPU/CPU\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "        metrics = compute_metrics(logits, y)\n",
    "        for key in total_metrics:\n",
    "            total_metrics[key] += metrics[key] * x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_metrics = {key: value / total_samples for key, value in total_metrics.items()}\n",
    "    return avg_loss, avg_metrics\n",
    "\n",
    "# Hàm đánh giá\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_metrics = {'accuracy': 0, 'f1_macro': 0}\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(loader, desc=\"Evaluating\"):\n",
    "            x, y = x.to(device), y.to(device)  # Chuyển dữ liệu lên GPU/CPU\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            total_samples += x.size(0)\n",
    "            metrics = compute_metrics(logits, y)\n",
    "            for key in total_metrics:\n",
    "                total_metrics[key] += metrics[key] * x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_metrics = {key: value / total_samples for key, value in total_metrics.items()}\n",
    "    return avg_loss, avg_metrics\n",
    "\n",
    "# Chuẩn bị dữ liệu và huấn luyện\n",
    "def main():\n",
    "    # Thiết lập thiết bị (GPU hoặc CPU)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    root_dir = \"../../../Agriculture-Vision-2021_processed_zip/trainrandcrop256/\"\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = CustomPTSegmentationDataset(root_dir)\n",
    "    indices = list(range(len(dataset)))\n",
    "    train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "    \n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Khởi tạo mô hình, loss và optimizer\n",
    "    model = SimpleCNN(num_classes=8).to(device)  # Chuyển mô hình lên GPU/CPU\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)  # Chuyển hàm mất mát lên GPU/CPU\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Huấn luyện và đánh giá\n",
    "    num_epochs = 20\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_metrics = train(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_metrics['accuracy']:.4f} | Train F1: {train_metrics['f1_macro']:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_metrics['accuracy']:.4f} | Val F1: {val_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67a474-eb0d-424e-9340-c7a95cda96f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Nas",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
