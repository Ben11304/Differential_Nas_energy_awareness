{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bb65be8-9dd3-4f08-8306-f7731dea3928",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.data_utils'; 'utils' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      6\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dl-energy-estimator\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m preprocess_and_normalize_energy_data, parse_codecarbon_output\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiments_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m split_data_set, fit_model, compute_log_transformed_features, apply_data_transforms, test_model\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# T·∫°o m·ªôt sample ƒë·ªÉ d·ª± ƒëo√°n\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.data_utils'; 'utils' is not a package"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append('./dl-energy-estimator')\n",
    "from utils.data_utils import preprocess_and_normalize_energy_data, parse_codecarbon_output\n",
    "from utils.experiments_utils import split_data_set, fit_model, compute_log_transformed_features, apply_data_transforms, test_model\n",
    "# T·∫°o m·ªôt sample ƒë·ªÉ d·ª± ƒëo√°n\n",
    "sample = pd.DataFrame([{\n",
    "    \"batch_size\": 123,\n",
    "    \"image_size\": 127,\n",
    "    \"kernel_size\": 7,\n",
    "    \"input_size\":4,\n",
    "    \"output_size\":2,\n",
    "    \"in_channels\": 80,\n",
    "    \"out_channels\": 29,\n",
    "    \"stride\": 1,\n",
    "    \"padding\": 2,\n",
    "    \"attributed\": \"conv2d\",\n",
    "    \"sub_attributed\": \"tanh\",\n",
    "    \"macs\":2.18534484e+11\n",
    "}])\n",
    "\n",
    "\n",
    "op_name=sample[\"attributed\"][0]\n",
    "if op_name==\"activation\":\n",
    "    param_cols = ['batch_size','input_size']\n",
    "    sample_norm=sample[param_cols]\n",
    "    data_linear_with_log, param_cols_with_log = sample_norm, param_cols\n",
    "    sub=sample[\"sub_attributed\"][0]\n",
    "    with open(f\"./energy_model/{op_name}/{sub}/linear_test_conv_model.pkl\", \"rb\") as f:\n",
    "        loaded_model = pickle.load(f)\n",
    "    # Load transformers\n",
    "    with open(f\"./energy_model/{op_name}/{sub}/x_transformer.pkl\", \"rb\") as f:\n",
    "        x_transformer = pickle.load(f)\n",
    "    \n",
    "    with open(f\"./energy_model/{op_name}/{sub}/y_transformer.pkl\", \"rb\") as f:\n",
    "        y_transformer = pickle.load(f)\n",
    "    \n",
    "    X_new_transformed = x_transformer.transform(data_linear_with_log)\n",
    "\n",
    "    # D·ª± ƒëo√°n v√† inverse scale (n·∫øu mu·ªën)\n",
    "    y_pred_scaled = loaded_model.predict(X_new_transformed)\n",
    "    y_pred = y_transformer.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
    "    \n",
    "    print(f\"üìå D·ª± ƒëo√°n nƒÉng l∆∞·ª£ng ti√™u th·ª•: {y_pred[0][0]}\")\n",
    "\n",
    "else:\n",
    "    if op_name==\"linear\":\n",
    "        param_cols = ['batch_size','input_size','output_size']\n",
    "    if op_name==\"maxpool2d\":\n",
    "        param_cols=['batch_size', 'image_size', 'kernel_size', 'in_channels', 'stride', 'padding']\n",
    "    if op_name==\"conv2d\":\n",
    "        param_cols = ['batch_size','image_size','kernel_size','in_channels','out_channels','stride','padding']\n",
    "        \n",
    "    sample_norm=sample[param_cols]\n",
    "    data_linear_with_log, param_cols_with_log = compute_log_transformed_features(sample_norm, param_cols)\n",
    "    data_linear_with_log['macs']=sample[\"macs\"]\n",
    "    \n",
    "    with open(f\"./energy_model/{op_name}/linear_test_conv_model.pkl\", \"rb\") as f:\n",
    "        loaded_model = pickle.load(f)\n",
    "    # Load transformers\n",
    "    with open(f\"./energy_model/{op_name}/x_transformer.pkl\", \"rb\") as f:\n",
    "        x_transformer = pickle.load(f)\n",
    "    \n",
    "    with open(f\"./energy_model/{op_name}/y_transformer.pkl\", \"rb\") as f:\n",
    "        y_transformer = pickle.load(f)\n",
    "    \n",
    "    X_new_transformed = x_transformer.transform(data_linear_with_log)\n",
    "    \n",
    "    # D·ª± ƒëo√°n v√† inverse scale (n·∫øu mu·ªën)\n",
    "    y_pred_scaled = loaded_model.predict(X_new_transformed)\n",
    "    y_pred = y_transformer.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
    "    \n",
    "    print(f\"üìå D·ª± ƒëo√°n nƒÉng l∆∞·ª£ng ti√™u th·ª•: {y_pred[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb8eedf-899c-4b2e-bfbf-9bf46bbe6f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "import logging\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.datasets as dset\n",
    "import torch.backends.cudnn as cudnn\n",
    "from model import NetworkCIFAR as Network\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "class CustomPTSegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        super().__init__()\n",
    "        self.images_dir = os.path.join(root_dir, \"X\")\n",
    "        self.labels_dir = os.path.join(root_dir, \"labels\")\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(self.images_dir, '*.pt')))\n",
    "        self.label_paths = [os.path.join(self.labels_dir, os.path.basename(p)) for p in self.image_paths]\n",
    "        assert all([os.path.exists(p) for p in self.label_paths]), \"Missing label files.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.load(self.image_paths[idx]).float().div(255.0)  # Normalize\n",
    "        label = torch.load(self.label_paths[idx]).float()\n",
    "        image = image.permute(0, 2, 1)  # [C, H, W]\n",
    "        label = label.permute(0, 2, 1)\n",
    "        if self.transform:\n",
    "            image, label = self.transform(image, label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a0c882f-0ec7-4248-8784-ed184fc181db",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = CustomPTSegmentationDataset(root_dir=\"../../../Agriculture-Vision-2021_processed_zip/trainrandcrop256/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "548a2dc6-b9b2-48da-8e5a-2182a4b696e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing statistics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:02<00:00, 249.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average image height: 512.00 pixels\n",
      "Average image width: 512.00 pixels\n",
      "\n",
      "Class pixel distribution:\n",
      "Class 0: 1091411 pixels (0.83%)\n",
      "Class 1: 15994505 pixels (12.20%)\n",
      "Class 2: 1082773 pixels (0.83%)\n",
      "Class 3: 9624748 pixels (7.34%)\n",
      "Class 4: 130532 pixels (0.10%)\n",
      "Class 5: 1992060 pixels (1.52%)\n",
      "Class 6: 534739 pixels (0.41%)\n",
      "Class 7: 7907236 pixels (6.03%)\n",
      "Class 8: 92967389 pixels (70.93%)\n",
      "\n",
      "Total pixels counted: 131325393\n",
      "Expected total pixels: 131072000.0\n",
      "Warning: Pixel count mismatch, possible data inconsistency.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CustomPTSegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        super().__init__()\n",
    "        self.images_dir = os.path.join(root_dir, \"X\")\n",
    "        self.labels_dir = os.path.join(root_dir, \"labels\")\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(self.images_dir, '*.pt')))\n",
    "        self.label_paths = [os.path.join(self.labels_dir, os.path.basename(p)) for p in self.image_paths]\n",
    "        assert all([os.path.exists(p) for p in self.label_paths]), \"Missing label files.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.load(self.image_paths[idx]).float().div(255.0)  # Normalize\n",
    "        label = torch.load(self.label_paths[idx]).float()\n",
    "        image = image.permute(0, 2, 1)  # [C, H, W]\n",
    "        label = label.permute(0, 2, 1)\n",
    "        if self.transform:\n",
    "            image, label = self.transform(image, label)\n",
    "        return image, label\n",
    "\n",
    "def compute_statistics(dataset):\n",
    "    total_samples = len(dataset)\n",
    "    print(f\"Total number of samples: {total_samples}\")\n",
    "\n",
    "    # Kh·ªüi t·∫°o bi·∫øn th·ªëng k√™\n",
    "    total_height = 0\n",
    "    total_width = 0\n",
    "    class_pixel_counts = torch.zeros(9, dtype=torch.long)  # 9 l·ªõp: 0-8\n",
    "    total_pixels = 0\n",
    "\n",
    "    # S·ª≠ d·ª•ng tqdm ƒë·ªÉ theo d√µi ti·∫øn tr√¨nh\n",
    "    with tqdm(total=total_samples, desc=\"Computing statistics\") as pbar:\n",
    "        for idx in range(total_samples):\n",
    "            try:\n",
    "                image, label = dataset[idx]\n",
    "\n",
    "                # K√≠ch th∆∞·ªõc ·∫£nh\n",
    "                _, H, W = image.shape\n",
    "                total_height += H\n",
    "                total_width += W\n",
    "\n",
    "                # ƒê·∫øm pixel theo l·ªõp\n",
    "                # label: [9, H, W], gi√° tr·ªã 0 ho·∫∑c 1 cho m·ªói l·ªõp\n",
    "                for class_idx in range(9):\n",
    "                    class_pixel_counts[class_idx] += torch.sum(label[class_idx] > 0).item()\n",
    "                total_pixels += H * W\n",
    "\n",
    "                pbar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # T√≠nh trung b√¨nh k√≠ch th∆∞·ªõc\n",
    "    avg_height = total_height / total_samples\n",
    "    avg_width = total_width / total_samples\n",
    "    print(f\"Average image height: {avg_height:.2f} pixels\")\n",
    "    print(f\"Average image width: {avg_width:.2f} pixels\")\n",
    "\n",
    "    # T√≠nh t·ª∑ l·ªá ph√¢n ph·ªëi l·ªõp\n",
    "    class_percentages = (class_pixel_counts.float() / total_pixels) * 100\n",
    "    print(\"\\nClass pixel distribution:\")\n",
    "    for class_idx in range(9):\n",
    "        print(f\"Class {class_idx}: {class_pixel_counts[class_idx]} pixels ({class_percentages[class_idx]:.2f}%)\")\n",
    "\n",
    "    # Ki·ªÉm tra t·ªïng s·ªë pixel\n",
    "    print(f\"\\nTotal pixels counted: {class_pixel_counts.sum().item()}\")\n",
    "    print(f\"Expected total pixels: {total_samples * avg_height * avg_width}\")\n",
    "    if abs(class_pixel_counts.sum().item() - (total_samples * avg_height * avg_width)) > 1e-5:\n",
    "        print(\"Warning: Pixel count mismatch, possible data inconsistency.\")\n",
    "\n",
    "    # Ki·ªÉm tra gi√° tr·ªã b·∫•t th∆∞·ªùng\n",
    "    for idx in range(total_samples):\n",
    "        try:\n",
    "            _, label = dataset[idx]\n",
    "            if torch.any(torch.isnan(label)) or torch.any(label < 0) or torch.any(label > 1):\n",
    "                print(f\"Warning: Invalid values detected in label at index {idx}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking sample {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = \"../../../Agriculture-Vision-2021_processed_zip/trainrandcrop256/\"\n",
    "    dataset = CustomPTSegmentationDataset(root_dir)\n",
    "    compute_statistics(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae3c923-5a7f-400e-a36d-0a21e1eb9cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing statistics and labels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [03:06<00:00, 26.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average image height: 256.00 pixels\n",
      "Average image width: 256.00 pixels\n",
      "\n",
      "Class pixel distribution:\n",
      "Double plant (Class 0): 2536671 pixels (0.77%)\n",
      "Drydown (Class 1): 41408797 pixels (12.64%)\n",
      "Endrow (Class 2): 3084536 pixels (0.94%)\n",
      "Nutrient deficiency (Class 3): 24665003 pixels (7.53%)\n",
      "Planter skip (Class 4): 653315 pixels (0.20%)\n",
      "Water (Class 5): 3476414 pixels (1.06%)\n",
      "Waterway (Class 6): 2160273 pixels (0.66%)\n",
      "Weed cluster (Class 7): 20049768 pixels (6.12%)\n",
      "background (Class 8): 230127092 pixels (70.23%)\n",
      "\n",
      "Total pixels counted: 328161869\n",
      "Expected total pixels: 327680000\n",
      "Warning: Pixel count mismatch, possible data inconsistency.\n",
      "\n",
      "Distribution of number of labels per image:\n",
      "Images with 1 label(s): 30 (0.60%)\n",
      "Images with 2 label(s): 4662 (93.24%)\n",
      "Images with 3 label(s): 273 (5.46%)\n",
      "Images with 4 label(s): 35 (0.70%)\n",
      "\n",
      "Sample label assignments (first 10 images):\n",
      "Image 0: ['Double plant', 'background'] (2 labels)\n",
      "Image 1: ['Double plant', 'background'] (2 labels)\n",
      "Image 2: ['Double plant', 'background'] (2 labels)\n",
      "Image 3: ['Double plant', 'background'] (2 labels)\n",
      "Image 4: ['Planter skip', 'background'] (2 labels)\n",
      "Image 5: ['Drydown', 'background'] (2 labels)\n",
      "Image 6: ['Drydown', 'Waterway', 'background'] (3 labels)\n",
      "Image 7: ['Endrow', 'background'] (2 labels)\n",
      "Image 8: ['Endrow', 'background'] (2 labels)\n",
      "Image 9: ['Drydown', 'background'] (2 labels)\n",
      "\n",
      "Checking for invalid label values...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class CustomPTSegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        super().__init__()\n",
    "        self.images_dir = os.path.join(root_dir, \"X\")\n",
    "        self.labels_dir = os.path.join(root_dir, \"labels\")\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(self.images_dir, '*.pt')))\n",
    "        self.label_paths = [os.path.join(self.labels_dir, os.path.basename(p)) for p in self.image_paths]\n",
    "        assert all([os.path.exists(p) for p in self.label_paths]), \"Missing label files.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.load(self.image_paths[idx]).float().div(255.0)  # Normalize\n",
    "        label = torch.load(self.label_paths[idx]).float()\n",
    "        image = image.permute(0, 2, 1)  # [C, H, W]\n",
    "        label = label.permute(0, 2, 1)  # [9, H, W]\n",
    "        if self.transform:\n",
    "            image, label = self.transform(image, label)\n",
    "        return image, label\n",
    "\n",
    "def compute_statistics_and_labels(dataset):\n",
    "    total_samples = len(dataset)\n",
    "    print(f\"Total number of samples: {total_samples}\")\n",
    "\n",
    "    # Kh·ªüi t·∫°o bi·∫øn th·ªëng k√™\n",
    "    total_height = 0\n",
    "    total_width = 0\n",
    "    class_pixel_counts = torch.zeros(9, dtype=torch.long)  # 9 l·ªõp: 0-8\n",
    "    total_pixels = 0\n",
    "    labels_per_image = []  # L∆∞u danh s√°ch c√°c l·ªõp c√≥ trong m·ªói ·∫£nh\n",
    "    label_count_distribution = np.zeros(10, dtype=int)  # ƒê·∫øm s·ªë ·∫£nh c√≥ 0, 1, ..., 9 nh√£n\n",
    "\n",
    "    # T√™n l·ªõp cho in ra k·∫øt qu·∫£\n",
    "    class_names = [\n",
    "        \"Double plant\", \"Drydown\", \"Endrow\", \"Nutrient deficiency\",\n",
    "        \"Planter skip\", \"Water\", \"Waterway\", \"Weed cluster\", \"background\"\n",
    "    ]\n",
    "\n",
    "    # S·ª≠ d·ª•ng tqdm ƒë·ªÉ theo d√µi ti·∫øn tr√¨nh\n",
    "    with tqdm(total=total_samples, desc=\"Computing statistics and labels\") as pbar:\n",
    "        for idx in range(total_samples):\n",
    "            try:\n",
    "                image, label = dataset[idx]\n",
    "\n",
    "                # K√≠ch th∆∞·ªõc ·∫£nh\n",
    "                _, H, W = image.shape\n",
    "                total_height += H\n",
    "                total_width += W\n",
    "\n",
    "                # ƒê·∫øm pixel theo l·ªõp v√† x√°c ƒë·ªãnh nh√£n c√≥ trong ·∫£nh\n",
    "                image_labels = []\n",
    "                for class_idx in range(9):\n",
    "                    pixel_count = torch.sum(label[class_idx] > 0).item()\n",
    "                    class_pixel_counts[class_idx] += pixel_count\n",
    "                    if pixel_count > 0:\n",
    "                        image_labels.append(class_idx)\n",
    "                total_pixels += H * W\n",
    "\n",
    "                # L∆∞u danh s√°ch nh√£n c·ªßa ·∫£nh n√†y\n",
    "                labels_per_image.append(image_labels)\n",
    "                # C·∫≠p nh·∫≠t ph√¢n ph·ªëi s·ªë l∆∞·ª£ng nh√£n\n",
    "                num_labels = len(image_labels)\n",
    "                label_count_distribution[num_labels] += 1\n",
    "\n",
    "                pbar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # T√≠nh trung b√¨nh k√≠ch th∆∞·ªõc\n",
    "    avg_height = total_height / total_samples\n",
    "    avg_width = total_width / total_samples\n",
    "    print(f\"\\nAverage image height: {avg_height:.2f} pixels\")\n",
    "    print(f\"Average image width: {avg_width:.2f} pixels\")\n",
    "\n",
    "    # T√≠nh t·ª∑ l·ªá ph√¢n ph·ªëi l·ªõp\n",
    "    class_percentages = (class_pixel_counts.float() / total_pixels) * 100\n",
    "    print(\"\\nClass pixel distribution:\")\n",
    "    for class_idx in range(9):\n",
    "        print(f\"{class_names[class_idx]} (Class {class_idx}): {class_pixel_counts[class_idx]} pixels ({class_percentages[class_idx]:.2f}%)\")\n",
    "\n",
    "    # Ki·ªÉm tra t·ªïng s·ªë pixel\n",
    "    print(f\"\\nTotal pixels counted: {class_pixel_counts.sum().item()}\")\n",
    "    print(f\"Expected total pixels: {total_samples * avg_height * avg_width:.0f}\")\n",
    "    if abs(class_pixel_counts.sum().item() - (total_samples * avg_height * avg_width)) > 1e-5:\n",
    "        print(\"Warning: Pixel count mismatch, possible data inconsistency.\")\n",
    "\n",
    "    # Th·ªëng k√™ ph√¢n ph·ªëi s·ªë l∆∞·ª£ng nh√£n m·ªói ·∫£nh\n",
    "    print(\"\\nDistribution of number of labels per image:\")\n",
    "    for num_labels in range(10):\n",
    "        if label_count_distribution[num_labels] > 0:\n",
    "            print(f\"Images with {num_labels} label(s): {label_count_distribution[num_labels]} ({label_count_distribution[num_labels]/total_samples*100:.2f}%)\")\n",
    "\n",
    "    # In danh s√°ch nh√£n cho m·ªôt s·ªë ·∫£nh m·∫´u (v√≠ d·ª•: 10 ·∫£nh ƒë·∫ßu)\n",
    "    print(\"\\nSample label assignments (first 10 images):\")\n",
    "    for idx in range(min(10, total_samples)):\n",
    "        if labels_per_image[idx]:\n",
    "            label_names = [class_names[i] for i in labels_per_image[idx]]\n",
    "            print(f\"Image {idx}: {label_names} ({len(label_names)} labels)\")\n",
    "        else:\n",
    "            print(f\"Image {idx}: No labels\")\n",
    "\n",
    "    # Ki·ªÉm tra gi√° tr·ªã b·∫•t th∆∞·ªùng\n",
    "    print(\"\\nChecking for invalid label values...\")\n",
    "    for idx in range(total_samples):\n",
    "        try:\n",
    "            _, label = dataset[idx]\n",
    "            if torch.any(torch.isnan(label)) or torch.any(label < 0) or torch.any(label > 1):\n",
    "                print(f\"Warning: Invalid values detected in label at index {idx}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking sample {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = \"../../../Agriculture-Vision-2021_processed_zip/trainrandcrop256/\"\n",
    "    dataset = CustomPTSegmentationDataset(root_dir)\n",
    "    compute_statistics_and_labels(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b659dd7-389d-4559-9a94-346a8851e1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing statistics and labels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30000/30000 [00:29<00:00, 1030.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average image height: 64.00 pixels\n",
      "Average image width: 64.00 pixels\n",
      "\n",
      "Class pixel distribution:\n",
      "Double plant (Class 0): 1107579 pixels (0.90%)\n",
      "Drydown (Class 1): 17110534 pixels (13.92%)\n",
      "Endrow (Class 2): 1192208 pixels (0.97%)\n",
      "Nutrient deficiency (Class 3): 9632847 pixels (7.84%)\n",
      "Planter skip (Class 4): 265605 pixels (0.22%)\n",
      "Water (Class 5): 1386593 pixels (1.13%)\n",
      "Waterway (Class 6): 1160899 pixels (0.94%)\n",
      "Weed cluster (Class 7): 8224195 pixels (6.69%)\n",
      "\n",
      "Number of images containing each class:\n",
      "Double plant (Class 0): 678 images (2.26%)\n",
      "Drydown (Class 1): 5368 images (17.89%)\n",
      "Endrow (Class 2): 737 images (2.46%)\n",
      "Nutrient deficiency (Class 3): 3252 images (10.84%)\n",
      "Planter skip (Class 4): 198 images (0.66%)\n",
      "Water (Class 5): 504 images (1.68%)\n",
      "Waterway (Class 6): 546 images (1.82%)\n",
      "Weed cluster (Class 7): 2989 images (9.96%)\n",
      "\n",
      "Total pixels counted: 40080460\n",
      "Expected total pixels: 122880000\n",
      "Warning: Pixel count mismatch, possible data inconsistency.\n",
      "\n",
      "Most common label combinations (top 5):\n",
      "No labels: 15871 images (52.90%)\n",
      "Combination ['Drydown']: 5363 images (17.88%)\n",
      "Combination ['Nutrient deficiency']: 3250 images (10.83%)\n",
      "Combination ['Weed cluster']: 2900 images (9.67%)\n",
      "Combination ['Endrow']: 648 images (2.16%)\n",
      "\n",
      "Checking for invalid label values...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class CustomPTSegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        super().__init__()\n",
    "        self.images_dir = os.path.join(root_dir, \"X\")\n",
    "        self.labels_dir = os.path.join(root_dir, \"labels\")\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(self.images_dir, '*.pt')))\n",
    "        self.label_paths = [os.path.join(self.labels_dir, os.path.basename(p)) for p in self.image_paths]\n",
    "        assert all([os.path.exists(p) for p in self.label_paths]), \"Missing label files.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.load(self.image_paths[idx]).float().div(255.0)  # Normalize\n",
    "        label = torch.load(self.label_paths[idx]).float()\n",
    "        image = image.permute(0, 2, 1)  # [C, H, W]\n",
    "        label = label.permute(0, 2, 1)  # [9, H, W]\n",
    "        if self.transform:\n",
    "            image, label = self.transform(image, label)\n",
    "        return image, label\n",
    "\n",
    "def compute_statistics_and_labels(dataset):\n",
    "    total_samples = len(dataset)\n",
    "    print(f\"Total number of samples: {total_samples}\")\n",
    "\n",
    "    # Kh·ªüi t·∫°o bi·∫øn th·ªëng k√™\n",
    "    total_height = 0\n",
    "    total_width = 0\n",
    "    class_pixel_counts = torch.zeros(8, dtype=torch.long)  # S·ªë pixel m·ªói l·ªõp\n",
    "    class_image_counts = torch.zeros(8, dtype=torch.long)  # S·ªë ·∫£nh ch·ª©a m·ªói l·ªõp\n",
    "    total_pixels = 0\n",
    "    label_combinations = []  # L∆∞u c√°c b·ªô nh√£n ƒë·ªÉ ph√¢n t√≠ch k·∫øt h·ª£p\n",
    "\n",
    "    # T√™n l·ªõp\n",
    "    class_names = [\n",
    "        \"Double plant\", \"Drydown\", \"Endrow\", \"Nutrient deficiency\",\n",
    "        \"Planter skip\", \"Water\", \"Waterway\", \"Weed cluster\"\n",
    "    ]\n",
    "\n",
    "    # S·ª≠ d·ª•ng tqdm ƒë·ªÉ theo d√µi ti·∫øn tr√¨nh\n",
    "    with tqdm(total=total_samples, desc=\"Computing statistics and labels\") as pbar:\n",
    "        for idx in range(total_samples):\n",
    "            try:\n",
    "                image, label = dataset[idx]\n",
    "\n",
    "                # K√≠ch th∆∞·ªõc ·∫£nh\n",
    "                _, H, W = image.shape\n",
    "                total_height += H\n",
    "                total_width += W\n",
    "\n",
    "                # ƒê·∫øm pixel v√† ki·ªÉm tra s·ª± hi·ªán di·ªán c·ªßa l·ªõp\n",
    "                image_labels = []\n",
    "                for class_idx in range(8):\n",
    "                    pixel_count = torch.sum(label[class_idx] > 0).item()\n",
    "                    class_pixel_counts[class_idx] += pixel_count\n",
    "                    if pixel_count > 0:\n",
    "                        class_image_counts[class_idx] += 1\n",
    "                        image_labels.append(class_idx)\n",
    "                total_pixels += H * W\n",
    "\n",
    "                # L∆∞u b·ªô nh√£n cho ph√¢n t√≠ch k·∫øt h·ª£p\n",
    "                label_combinations.append(tuple(sorted(image_labels)))\n",
    "\n",
    "                pbar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # T√≠nh trung b√¨nh k√≠ch th∆∞·ªõc\n",
    "    avg_height = total_height / total_samples\n",
    "    avg_width = total_width / total_samples\n",
    "    print(f\"\\nAverage image height: {avg_height:.2f} pixels\")\n",
    "    print(f\"Average image width: {avg_width:.2f} pixels\")\n",
    "\n",
    "    # T√≠nh t·ª∑ l·ªá ph√¢n ph·ªëi pixel theo l·ªõp\n",
    "    class_pixel_percentages = (class_pixel_counts.float() / total_pixels) * 100\n",
    "    print(\"\\nClass pixel distribution:\")\n",
    "    for class_idx in range(8):\n",
    "        print(f\"{class_names[class_idx]} (Class {class_idx}): {class_pixel_counts[class_idx]} pixels ({class_pixel_percentages[class_idx]:.2f}%)\")\n",
    "\n",
    "    # Th·ªëng k√™ s·ªë ·∫£nh ch·ª©a m·ªói l·ªõp\n",
    "    class_image_percentages = (class_image_counts.float() / total_samples) * 100\n",
    "    print(\"\\nNumber of images containing each class:\")\n",
    "    for class_idx in range(8):\n",
    "        print(f\"{class_names[class_idx]} (Class {class_idx}): {class_image_counts[class_idx]} images ({class_image_percentages[class_idx]:.2f}%)\")\n",
    "\n",
    "    # Ki·ªÉm tra t·ªïng s·ªë pixel\n",
    "    print(f\"\\nTotal pixels counted: {class_pixel_counts.sum().item()}\")\n",
    "    print(f\"Expected total pixels: {total_samples * avg_height * avg_width:.0f}\")\n",
    "    if abs(class_pixel_counts.sum().item() - (total_samples * avg_height * avg_width)) > 1e-5:\n",
    "        print(\"Warning: Pixel count mismatch, possible data inconsistency.\")\n",
    "\n",
    "    # (T√πy ch·ªçn) Th·ªëng k√™ c√°c k·∫øt h·ª£p nh√£n ph·ªï bi·∫øn\n",
    "    print(\"\\nMost common label combinations (top 5):\")\n",
    "    combination_counts = Counter(label_combinations)\n",
    "    for comb, count in combination_counts.most_common(5):\n",
    "        if comb:\n",
    "            comb_names = [class_names[i] for i in comb]\n",
    "            print(f\"Combination {comb_names}: {count} images ({count/total_samples*100:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"No labels: {count} images ({count/total_samples*100:.2f}%)\")\n",
    "\n",
    "    # Ki·ªÉm tra gi√° tr·ªã b·∫•t th∆∞·ªùng\n",
    "    print(\"\\nChecking for invalid label values...\")\n",
    "    for idx in range(total_samples):\n",
    "        try:\n",
    "            _, label = dataset[idx]\n",
    "            if torch.any(torch.isnan(label)) or torch.any(label < 0) or torch.any(label > 1):\n",
    "                print(f\"Warning: Invalid values detected in label at index {idx}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking sample {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = \"../../../Agriculture-Vision-2021_processed_zip/trainrandcrop256/\"\n",
    "    dataset = CustomPTSegmentationDataset(root_dir)\n",
    "    compute_statistics_and_labels(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2e9a56-bae4-4d21-98c3-109ad1ada322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train:  38%|‚ñà‚ñà‚ñà‚ñä      | 11281/30000 [31:06<35:36,  8.76it/s]  IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing train:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 18327/30000 [53:14<46:02,  4.23it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "import logging\n",
    "\n",
    "# C·∫•u h√¨nh logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def is_binary_image(image_array):\n",
    "    \"\"\"Ki·ªÉm tra xem ·∫£nh c√≥ ph·∫£i nh·ªã ph√¢n (ch·ªâ ch·ª©a 0 v√†/ho·∫∑c 1) kh√¥ng.\"\"\"\n",
    "    unique_values = np.unique(image_array)\n",
    "    return set(unique_values).issubset({0, 1})\n",
    "\n",
    "def normalize_if_not_binary(image_array):\n",
    "    \"\"\"Chu·∫©n h√≥a ·∫£nh v·ªÅ [0, 1] n·∫øu kh√¥ng ph·∫£i nh·ªã ph√¢n.\"\"\"\n",
    "    if not is_binary_image(image_array):\n",
    "        return image_array / 255.0\n",
    "    return image_array.astype(np.float32) / 255.0  # Chuy·ªÉn v·ªÅ [0, 1] ngay c·∫£ khi nh·ªã ph√¢n\n",
    "\n",
    "def find_image_path(root_dir, image_name_base):\n",
    "    exts = ['png', 'jpg', 'jpeg']\n",
    "    for ext in exts:\n",
    "        path = os.path.join(root_dir, f\"{image_name_base}.{ext}\")\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "    raise FileNotFoundError(f\"File {image_name_base}.[png/jpg/jpeg] not found in {root_dir}\")\n",
    "\n",
    "def random_crop(img_tensor, label_tensor, region_mask_tensor, crop_size=(256, 256)):\n",
    "    _, H, W = img_tensor.shape\n",
    "    ch, cw = crop_size\n",
    "\n",
    "    valid_positions = (region_mask_tensor > 0).nonzero(as_tuple=False)\n",
    "\n",
    "    if len(valid_positions) == 0:\n",
    "        top = (H - ch) // 2\n",
    "        left = (W - cw) // 2\n",
    "    else:\n",
    "        idx = random.randint(0, len(valid_positions) - 1)\n",
    "        center_y, center_x = valid_positions[idx, 0].item(), valid_positions[idx, 1].item()\n",
    "        top = max(0, min(H - ch, center_y - ch // 2))\n",
    "        left = max(0, min(W - cw, center_x - cw // 2))\n",
    "\n",
    "    img_cropped = TF.crop(img_tensor, top, left, ch, cw)\n",
    "    label_cropped = TF.crop(label_tensor, top, left, ch, cw)\n",
    "    mask_cropped = TF.crop(region_mask_tensor.unsqueeze(0), top, left, ch, cw).squeeze(0)\n",
    "\n",
    "    img_cropped = img_cropped * mask_cropped.unsqueeze(0)\n",
    "    label_cropped = label_cropped * mask_cropped.unsqueeze(0)\n",
    "\n",
    "    return img_cropped, label_cropped\n",
    "\n",
    "def load_sample(image_name, image_dir, mask_dir, boundary_dir, label_root, label_class_names, crop_size=(256, 256)):\n",
    "    image_name_base = os.path.splitext(image_name)[0]\n",
    "\n",
    "    try:\n",
    "        # Load RGB\n",
    "        image_path = find_image_path(image_dir, image_name_base)\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        img_array = np.array(img)\n",
    "        img_4ch = normalize_if_not_binary(img_array)  # Chu·∫©n h√≥a RGB n·∫øu c·∫ßn\n",
    "\n",
    "        # Load NIR\n",
    "        nir_path = image_path.replace('/rgb/', '/nir/')\n",
    "        nir_path = find_image_path(os.path.dirname(nir_path), image_name_base)\n",
    "        nir = Image.open(nir_path).convert('L')\n",
    "        nir_array = np.array(nir)\n",
    "        nir_normalized = normalize_if_not_binary(nir_array)  # Chu·∫©n h√≥a NIR n·∫øu c·∫ßn\n",
    "        img_4ch = np.concatenate([img_4ch, nir_normalized[:, :, None]], axis=-1)\n",
    "\n",
    "        # Load boundary and mask\n",
    "        boundary_path = find_image_path(boundary_dir, image_name_base)\n",
    "        mask_path = find_image_path(mask_dir, image_name_base)\n",
    "        boundary = np.array(Image.open(boundary_path)) > 0\n",
    "        valid_mask = np.array(Image.open(mask_path)) > 0\n",
    "        region_mask = boundary & valid_mask\n",
    "\n",
    "        # Load each class label\n",
    "        label_stack = []\n",
    "        for class_name in label_class_names:\n",
    "            label_class_path = os.path.join(label_root, class_name)\n",
    "            label_path = find_image_path(label_class_path, image_name_base)\n",
    "            label = np.array(Image.open(label_path).convert('L'))  # ƒê·∫£m b·∫£o l√† ·∫£nh x√°m\n",
    "            label_normalized = normalize_if_not_binary(label)\n",
    "            label_stack.append(label_normalized)\n",
    "\n",
    "        label_stack = np.stack(label_stack, axis=-1)\n",
    "\n",
    "        # Convert to tensors\n",
    "        img_tensor = torch.from_numpy(img_4ch).permute(2, 0, 1).float()  # [4, H, W], float32\n",
    "        label_tensor = torch.from_numpy(label_stack).permute(2, 0, 1).float()  # [num_classes, H, W]\n",
    "        region_mask_tensor = torch.from_numpy(region_mask.astype(np.uint8))  # [H, W]\n",
    "\n",
    "        # Random crop\n",
    "        img_tensor, label_tensor = random_crop(img_tensor, label_tensor, region_mask_tensor, crop_size=crop_size)\n",
    "        # print(np.shape(label_tensor))\n",
    "        # print(np.shape(img_tensor))\n",
    "        # Add background\n",
    "        # background = (label_tensor.sum(dim=0, keepdim=True) < 1e-6).float()  # Sum over classes, keep shape [1, H, W]\n",
    "        # gt_binary = torch.cat([label_tensor, background], dim=0)  # Shape: [num_classes + 1, H, W]\n",
    "        # if torch.any(gt_binary.sum(dim=0) < 1e-6):  # Check per pixel\n",
    "        #     print(\"Some pixels in gt_binary have no labels, even after adding background\")\n",
    "\n",
    "        return img_tensor, label_tensor\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading sample {image_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "def process_dataset_folder(\n",
    "    base_dir,\n",
    "    save_dir,\n",
    "    save_format=\"pt\",  # or \"npy\"\n",
    "    crop_size=(256, 256),\n",
    "    max_samples=500  # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng sample\n",
    "):\n",
    "    img_dir = os.path.join(base_dir, \"images/rgb\")\n",
    "    mask_dir = os.path.join(base_dir, \"masks\")\n",
    "    boundary_dir = os.path.join(base_dir, \"boundaries\")\n",
    "    label_root = os.path.join(base_dir, \"labels\")\n",
    "\n",
    "    label_class_names = sorted([\n",
    "        d for d in os.listdir(label_root)\n",
    "        if os.path.isdir(os.path.join(label_root, d))\n",
    "    ])\n",
    "\n",
    "    os.makedirs(os.path.join(save_dir, \"X\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir, \"labels\"), exist_ok=True)\n",
    "\n",
    "    image_files = [f for f in os.listdir(img_dir) if f.endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "    image_files = image_files[:min(max_samples, len(image_files))]  # Gi·ªõi h·∫°n s·ªë sample\n",
    "\n",
    "    for image_name in tqdm(image_files, desc=f\"Processing {os.path.basename(base_dir)}\"):\n",
    "        try:\n",
    "            img_tensor, gt_binary = load_sample(\n",
    "                image_name, img_dir, mask_dir, boundary_dir, label_root, label_class_names, crop_size=crop_size\n",
    "            )\n",
    "            name_base = os.path.splitext(image_name)[0]\n",
    "            if save_format == \"pt\":\n",
    "                torch.save(img_tensor, os.path.join(save_dir, \"X\", f\"{name_base}.pt\"))\n",
    "                torch.save(gt_binary, os.path.join(save_dir, \"labels\", f\"{name_base}.pt\"))\n",
    "            elif save_format == \"npy\":\n",
    "                np.save(os.path.join(save_dir, \"X\", f\"{name_base}.npy\"), img_tensor.numpy())\n",
    "                np.save(os.path.join(save_dir, \"labels\", f\"{name_base}.npy\"), gt_binary.numpy())\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error processing {image_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"../../../Agriculture-Vision-2021/train\"\n",
    "    save_dir = \"../../../Agriculture-Vision-2021_processed_zip/trainrandcrop256/\"\n",
    "    process_dataset_folder(base_dir, save_dir, save_format=\"pt\", crop_size=(64, 64), max_samples=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c61288c-675e-45af-8072-2e4f823845e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting labels to vectors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30000/30000 [00:32<00:00, 924.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 30000 files successfully.\n",
      "Saved vector labels to: ../../../Agriculture-Vision-2021_processed_zip/trainrandcrop256/vector_label\n",
      "Number of vector label files saved: 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CustomPTSegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        super().__init__()\n",
    "        self.images_dir = os.path.join(root_dir, \"X\")\n",
    "        self.labels_dir = os.path.join(root_dir, \"labels\")\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(self.images_dir, '*.pt')))\n",
    "        self.label_paths = [os.path.join(self.labels_dir, os.path.basename(p)) for p in self.image_paths]\n",
    "        assert all([os.path.exists(p) for p in self.label_paths]), \"Missing label files.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.load(self.image_paths[idx]).float().div(255.0)  # Normalize\n",
    "        label = torch.load(self.label_paths[idx]).float()\n",
    "        image = image.permute(0, 2, 1)  # [C, H, W]\n",
    "        label = label.permute(0, 2, 1)  # [9, H, W]\n",
    "        if self.transform:\n",
    "            image, label = self.transform(image, label)\n",
    "        return image, label\n",
    "\n",
    "def convert_labels_to_vector(root_dir):\n",
    "    # T·∫°o th∆∞ m·ª•c vector_label trong root_dir\n",
    "    vector_label_dir = os.path.join(root_dir, \"vector_label\")\n",
    "    os.makedirs(vector_label_dir, exist_ok=True)\n",
    "\n",
    "    # Kh·ªüi t·∫°o dataset\n",
    "    dataset = CustomPTSegmentationDataset(root_dir)\n",
    "    total_samples = len(dataset)\n",
    "    print(f\"Total number of samples: {total_samples}\")\n",
    "\n",
    "    processed_files = 0\n",
    "    errors = 0\n",
    "\n",
    "    # S·ª≠ d·ª•ng tqdm ƒë·ªÉ theo d√µi ti·∫øn tr√¨nh\n",
    "    with tqdm(total=total_samples, desc=\"Converting labels to vectors\") as pbar:\n",
    "        for idx in range(total_samples):\n",
    "            try:\n",
    "                # Ch·ªâ c·∫ßn nh√£n\n",
    "                _, label = dataset[idx]  # label: [8, H, W]\n",
    "\n",
    "                # Chuy·ªÉn tensor nh√£n th√†nh vector nh√£n\n",
    "                vector_label = torch.zeros(8, dtype=torch.float)\n",
    "                for class_idx in range(8):\n",
    "                    if torch.sum(label[class_idx] > 0).item() > 0:\n",
    "                        vector_label[class_idx] = 1\n",
    "\n",
    "                # L∆∞u vector nh√£n v√†o file .pt\n",
    "                output_path = os.path.join(vector_label_dir, os.path.basename(dataset.label_paths[idx]))\n",
    "                torch.save(vector_label, output_path)\n",
    "\n",
    "                processed_files += 1\n",
    "                pbar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {idx} ({dataset.label_paths[idx]}): {e}\")\n",
    "                errors += 1\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "    # B√°o c√°o k·∫øt qu·∫£\n",
    "    print(f\"\\nProcessed {processed_files} files successfully.\")\n",
    "    if errors > 0:\n",
    "        print(f\"Encountered {errors} errors.\")\n",
    "    print(f\"Saved vector labels to: {vector_label_dir}\")\n",
    "\n",
    "    # Ki·ªÉm tra s·ªë file l∆∞u ƒë∆∞·ª£c\n",
    "    saved_files = len(glob.glob(os.path.join(vector_label_dir, '*.pt')))\n",
    "    print(f\"Number of vector label files saved: {saved_files}\")\n",
    "    if saved_files != processed_files:\n",
    "        print(\"Warning: Number of saved files does not match processed files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = \"../../../Agriculture-Vision-2021_processed_zip/trainrandcrop256/\"\n",
    "    convert_labels_to_vector(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a6c4fb-f77e-45f1-810b-354b8a38e925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/12000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 131072]' is invalid for input of size 16384",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 151\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 143\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m--> 143\u001b[0m     train_loss, train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     val_loss, val_metrics \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, device)\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 78\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     76\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Chuy·ªÉn d·ªØ li·ªáu l√™n GPU/CPU\u001b[39;00m\n\u001b[1;32m     77\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 78\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, y)\n\u001b[1;32m     80\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Nas/darts_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Nas/darts_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m, in \u001b[0;36mSimpleCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[0;32m---> 25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Flatten\u001b[39;00m\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[1;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)  \u001b[38;5;66;03m# Logits ƒë·∫ßu ra\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 131072]' is invalid for input of size 16384"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a m√¥ h√¨nh CNN ƒë∆°n gi·∫£n\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        # Gi·∫£ s·ª≠ k√≠ch th∆∞·ªõc ·∫£nh ƒë·∫ßu v√†o l√† 256x256\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, 128)  # Sau 2 l·∫ßn pooling: 256 -> 128 -> 64\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 64 * 64)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # Logits ƒë·∫ßu ra\n",
    "        return x\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a dataset (t√°i s·ª≠ d·ª•ng t·ª´ code c·ªßa b·∫°n)\n",
    "class CustomPTSegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        super().__init__()\n",
    "        self.images_dir = os.path.join(root_dir, \"X\")\n",
    "        self.labels_dir = os.path.join(root_dir, \"vector_label\")\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(self.images_dir, '*.pt')))\n",
    "        self.label_paths = [os.path.join(self.labels_dir, os.path.basename(p)) for p in self.image_paths]\n",
    "        assert all([os.path.exists(p) for p in self.label_paths]), \"Missing label files.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.load(self.image_paths[idx]).float()\n",
    "        label = torch.load(self.label_paths[idx]).float()  # Gi·ªõi h·∫°n ·ªü 8 l·ªõp ƒë·∫ßu ti√™n\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# H√†m t√≠nh metrics\n",
    "def compute_metrics(preds, labels, threshold=0.5):\n",
    "    preds = torch.sigmoid(preds) > threshold\n",
    "    labels = labels > 0.5\n",
    "    tp = (preds & labels).sum(dim=0).float()\n",
    "    fp = (preds & ~labels).sum(dim=0).float()\n",
    "    fn = (~preds & labels).sum(dim=0).float()\n",
    "    precision = tp / (tp + fp + 1e-6)\n",
    "    recall = tp / (tp + fn + 1e-6)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "    accuracy = (preds == labels).float().mean(dim=0)\n",
    "    return {\n",
    "        'accuracy': accuracy.mean().item(),\n",
    "        'f1_macro': f1.mean().item()\n",
    "    }\n",
    "\n",
    "# H√†m hu·∫•n luy·ªán\n",
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_metrics = {'accuracy': 0, 'f1_macro': 0}\n",
    "    total_samples = 0\n",
    "\n",
    "    for x, y in tqdm(loader, desc=\"Training\"):\n",
    "        x, y = x.to(device), y.to(device)  # Chuy·ªÉn d·ªØ li·ªáu l√™n GPU/CPU\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "        metrics = compute_metrics(logits, y)\n",
    "        for key in total_metrics:\n",
    "            total_metrics[key] += metrics[key] * x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_metrics = {key: value / total_samples for key, value in total_metrics.items()}\n",
    "    return avg_loss, avg_metrics\n",
    "\n",
    "# H√†m ƒë√°nh gi√°\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_metrics = {'accuracy': 0, 'f1_macro': 0}\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(loader, desc=\"Evaluating\"):\n",
    "            x, y = x.to(device), y.to(device)  # Chuy·ªÉn d·ªØ li·ªáu l√™n GPU/CPU\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            total_samples += x.size(0)\n",
    "            metrics = compute_metrics(logits, y)\n",
    "            for key in total_metrics:\n",
    "                total_metrics[key] += metrics[key] * x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_metrics = {key: value / total_samples for key, value in total_metrics.items()}\n",
    "    return avg_loss, avg_metrics\n",
    "\n",
    "# Chu·∫©n b·ªã d·ªØ li·ªáu v√† hu·∫•n luy·ªán\n",
    "def main():\n",
    "    # Thi·∫øt l·∫≠p thi·∫øt b·ªã (GPU ho·∫∑c CPU)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    root_dir = \"../../../Agriculture-Vision-2021_processed_zip/trainrandcrop256/\"\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = CustomPTSegmentationDataset(root_dir)\n",
    "    indices = list(range(len(dataset)))\n",
    "    train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "    \n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Kh·ªüi t·∫°o m√¥ h√¨nh, loss v√† optimizer\n",
    "    model = SimpleCNN(num_classes=8).to(device)  # Chuy·ªÉn m√¥ h√¨nh l√™n GPU/CPU\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)  # Chuy·ªÉn h√†m m·∫•t m√°t l√™n GPU/CPU\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Hu·∫•n luy·ªán v√† ƒë√°nh gi√°\n",
    "    num_epochs = 20\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_metrics = train(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_metrics['accuracy']:.4f} | Train F1: {train_metrics['f1_macro']:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_metrics['accuracy']:.4f} | Val F1: {val_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67a474-eb0d-424e-9340-c7a95cda96f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Nas",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
